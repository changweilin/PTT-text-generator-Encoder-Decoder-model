{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "D72meokFwX7J",
    "outputId": "420d76a4-edf0-492d-9ee9-07b6933c685b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from C:\\Users\\User\\Anaconda3\\Lib\\site-packages\\jieba\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.ufa6ae29b0cbce8b45e006c7fa30eaaf8.cache\n",
      "Loading model cost 0.959 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199446\n",
      "1 的 的 479528 108422\n",
      "2 我 我 187622 60294\n",
      "3 <GO> <GO> 172800 172800\n",
      "4 <EOS> <EOS> 172800 172800\n",
      "5 是 是 157302 68972\n",
      "6 了 了 129931 60543\n",
      "7 有 有 108041 59614\n",
      "8 在 在 97066 54361\n",
      "9 也 也 86587 48759\n",
      "10 都 都 75404 45560\n",
      "11 就 就 71400 42410\n",
      "12 不 不 66238 42797\n",
      "13 很 很 54725 34820\n",
      "14 會 會 54210 36201\n",
      "15 他 他 49560 20170\n",
      "16 跟 跟 48539 31439\n",
      "17 看 看 48476 30452\n",
      "18 你 你 48168 21707\n",
      "19 想 想 47770 34168\n",
      "1000 跳 跳 1246 1022\n",
      "2000 整合 整合 607 489\n",
      "3000 往往 往往 366 345\n",
      "4000 007 007 255 156\n",
      "5000 處於 處於 192 188\n",
      "6000 一般來說 一般來說 154 150\n",
      "7000 何 何 125 101\n",
      "8000 有鬼 有鬼 105 100\n",
      "9000 備 備 89 89\n",
      "10000 半年前 半年前 78 78\n",
      "11000 掉下來 掉下來 68 66\n",
      "12000 托爾 托爾 61 46\n",
      "13000 傻子 傻子 54 51\n",
      "14000 活得 活得 49 48\n",
      "15000 寫給 寫給 44 41\n",
      "16000 希斯 希斯 41 31\n",
      "17000 毆打 毆打 37 33\n",
      "18000 編制 編制 34 29\n",
      "19000 激怒 激怒 32 30\n",
      "20000 神秘性 神秘性 30 5\n",
      "21000 睡意 睡意 27 27\n",
      "22000 真情 真情 26 24\n",
      "23000 寫些 寫些 24 24\n",
      "24000 野蠻女友 野蠻女友 23 14\n",
      "25000 20th 20th 21 17\n",
      "26000 世家 世家 20 20\n",
      "27000 天降 天降 19 18\n",
      "28000 人卡 人卡 18 13\n",
      "29000 決定權 決定權 17 17\n",
      "30000 始於 始於 16 15\n",
      "31000 初號機 初號機 16 10\n",
      "32000 早睡早起 早睡早起 15 15\n",
      "33000 記號 記號 14 13\n",
      "34000 內力 內力 14 11\n",
      "35000 大蓋 大蓋 13 12\n",
      "36000 冬訓 冬訓 12 12\n",
      "37000 著書 著書 12 12\n",
      "38000 反璞歸真 反璞歸真 11 11\n",
      "39000 皮皮 皮皮 11 8\n",
      "40000 內布拉斯加 內布拉斯加 11 7\n",
      "41000 之翼 之翼 10 10\n",
      "42000 克蘿伊摩蕾茲 克蘿伊摩蕾茲 10 8\n",
      "43000 高崎 高崎 9 5\n",
      "44000 無恥之徒 無恥之徒 9 9\n",
      "45000 散光 散光 9 6\n",
      "46000 XV XV 9 9\n",
      "47000 一王 一王 8 8\n",
      "48000 瑞士刀 瑞士刀 8 4\n",
      "49000 驚字塔 驚字塔 8 8\n",
      "50000 祝您 祝您 8 7\n",
      "51000 先列 先列 7 7\n",
      "52000 先拍 先拍 7 6\n",
      "53000 犯生 犯生 7 7\n",
      "54000 毒發 毒發 7 7\n",
      "55000 水天 水天 6 3\n",
      "56000 隱居 隱居 6 6\n",
      "57000 督個 督個 6 4\n",
      "58000 不太穩 不太穩 6 6\n",
      "59000 錢欣郁 錢欣郁 6 3\n",
      "60000 鑑別 鑑別 6 6\n",
      "61000 歐女 歐女 6 2\n",
      "62000 南丁格爾 南丁格爾 5 5\n",
      "63000 ZARA ZARA 5 5\n",
      "64000 看食 看食 5 5\n",
      "65000 同人小說 同人小說 5 5\n",
      "66000 Hynix Hynix 5 5\n",
      "67000 入浴 入浴 5 4\n",
      "68000 說凜 說凜 5 5\n",
      "69000 還會將 還會將 4 4\n",
      "70000 了然 了然 4 4\n",
      "71000 找錢 找錢 4 4\n",
      "72000 三番 三番 4 4\n",
      "73000 跑者 跑者 4 4\n",
      "74000 Innovation Innovation 4 4\n",
      "75000 組起來 組起來 4 4\n",
      "76000 學不太到 學不太到 4 4\n",
      "77000 鳥氣 鳥氣 4 4\n",
      "78000 武術指導 武術指導 4 4\n",
      "79000 星科晶朋 星科晶朋 4 4\n",
      "80000 dia dia 4 2\n",
      "81000 賴還 賴還 4 4\n",
      "82000 人拿錯 人拿錯 4 4\n",
      "83000 推新 推新 3 3\n",
      "84000 惟此 惟此 3 3\n",
      "85000 蠻近 蠻近 3 3\n",
      "86000 正賽 正賽 3 3\n",
      "87000 唇槍 唇槍 3 3\n",
      "88000 函給 函給 3 3\n",
      "89000 雨女 雨女 3 3\n",
      "90000 狂問 狂問 3 3\n",
      "91000 沒笑點 沒笑點 3 3\n",
      "92000 不掛 不掛 3 3\n",
      "93000 患憂 患憂 3 3\n",
      "94000 Ginoisgood Ginoisgood 3 3\n",
      "95000 thought thought 3 3\n",
      "96000 金陽 金陽 3 3\n",
      "97000 騷靈 騷靈 3 3\n",
      "98000 ATK ATK 3 3\n",
      "99000 realtimenews realtimenews 3 3\n",
      "100000 上展 上展 3 2\n",
      "101000 軟而 軟而 3 3\n",
      "102000 性文多 性文多 3 3\n",
      "103000 狂沙中 狂沙中 2 2\n",
      "104000 友尚 友尚 2 2\n",
      "105000 上序 上序 2 2\n",
      "106000 可增 可增 2 2\n",
      "107000 伊神 伊神 2 2\n",
      "108000 總覽 總覽 2 1\n",
      "109000 大可大辣 大可大辣 2 2\n",
      "110000 Karsa Karsa 2 2\n",
      "111000 老沙 老沙 2 2\n",
      "112000 三連霸 三連霸 2 2\n",
      "113000 人錢 人錢 2 2\n",
      "114000 小縫 小縫"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 2\n",
      "115000 heartdrunken heartdrunken 2 2\n",
      "116000 而妮 而妮 2 2\n",
      "117000 降職 降職 2 1\n",
      "118000 弄手 弄手 2 2\n",
      "119000 跟織 跟織 2 2\n",
      "120000 那聲 那聲 2 2\n",
      "121000 臭氣沖天 臭氣沖天 2 2\n",
      "122000 約聘職 約聘職 2 2\n",
      "123000 亞伯特 亞伯特 2 2\n",
      "124000 母子之情 母子之情 2 2\n",
      "125000 一撥 一撥 2 2\n",
      "126000 foreigner foreigner 2 2\n",
      "127000 沒兵 沒兵 2 2\n",
      "128000 速到 速到 2 2\n",
      "129000 119999999119999991199911999999911991199 119999999119999991199911999999911991199 2 2\n",
      "130000 愛主 愛主 2 2\n",
      "131000 業務經理 業務經理 2 2\n",
      "132000 舒涵 舒涵 2 2\n",
      "133000 浮台 浮台 2 1\n",
      "134000 和吳 和吳 2 2\n",
      "135000 Gatebox Gatebox 2 1\n",
      "136000 whatthefack whatthefack 2 2\n",
      "137000 邊默數 邊默數 2 2\n",
      "138000 李芳 李芳 2 1\n",
      "139000 群后 群后 2 2\n",
      "140000 找外 找外 2 2\n",
      "141000 防空壕 防空壕 2 2\n",
      "142000 道德哲學 道德哲學 1 1\n",
      "143000 Footlight Footlight 1 1\n",
      "144000 Ds Ds 1 1\n",
      "145000 全廢 全廢 1 1\n",
      "146000 喪夫 喪夫 1 1\n",
      "147000 櫻看 櫻看 1 1\n",
      "148000 DOCtRINE DOCtRINE 1 1\n",
      "149000 曼今 曼今 1 1\n",
      "150000 NanoDesu NanoDesu 1 1\n",
      "151000 不佳來 不佳來 1 1\n",
      "152000 重禮 重禮 1 1\n",
      "153000 帶冷 帶冷 1 1\n",
      "154000 繳為 繳為 1 1\n",
      "155000 乃讀 乃讀 1 1\n",
      "156000 附好 附好 1 1\n",
      "157000 無病 無病 1 1\n",
      "158000 1JPmnQcb 1JPmnQcb 1 1\n",
      "159000 他博 他博 1 1\n",
      "160000 歸頭 歸頭 1 1\n",
      "161000 診斷書 診斷書 1 1\n",
      "162000 腳刀 腳刀 1 1\n",
      "163000 比眾 比眾 1 1\n",
      "164000 Awakening Awakening 1 1\n",
      "165000 過卻 過卻 1 1\n",
      "166000 懂講 懂講 1 1\n",
      "167000 Goodwin Goodwin 1 1\n",
      "168000 Therese Therese 1 1\n",
      "169000 歪曲 歪曲 1 1\n",
      "170000 Nawi Nawi 1 1\n",
      "171000 晃棄 晃棄 1 1\n",
      "172000 測試版 測試版 1 1\n",
      "173000 到明 到明 1 1\n",
      "174000 從卡池 從卡池 1 1\n",
      "175000 原美緒 原美緒 1 1\n",
      "176000 戰速 戰速 1 1\n",
      "177000 Ciotta Ciotta 1 1\n",
      "178000 皮有 皮有 1 1\n",
      "179000 頭用 頭用 1 1\n",
      "180000 雙雄片 雙雄片 1 1\n",
      "181000 還辜 還辜 1 1\n",
      "182000 亂當 亂當 1 1\n",
      "183000 給我畫 給我畫 1 1\n",
      "184000 小呆鼠 小呆鼠 1 1\n",
      "185000 買才行 買才行 1 1\n",
      "186000 Trends Trends 1 1\n",
      "187000 立項 立項 1 1\n",
      "188000 岬灣 岬灣 1 1\n",
      "189000 f15 f15 1 1\n",
      "190000 資源配置 資源配置 1 1\n",
      "191000 ahqgogo ahqgogo 1 1\n",
      "192000 Omaya Omaya 1 1\n",
      "193000 人點 人點 1 1\n",
      "194000 不愛買 不愛買 1 1\n",
      "195000 船速 船速 1 1\n",
      "196000 白花油 白花油 1 1\n",
      "197000 人多活 人多活 1 1\n",
      "198000 櫻會 櫻會 1 1\n",
      "199000 偶戲 偶戲 1 1\n"
     ]
    }
   ],
   "source": [
    "# PTT title generator from content base on seq2seq model.\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import types \n",
    "\n",
    "#Initialize for files name and path.\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "totalboard_name = 'TJ-BG-AT-SX-CC-MV'\n",
    "board_name = ['Tech_job','Boy-Girl','AllTogether','sex','C_Chat','movie']\n",
    "board_dict_index = {'Tech_job':0,'Boy-Girl':1,'AllTogether':2,'sex':3,'C_Chat':4,'movie':5}\n",
    "max_word_length = 256\n",
    "min_word_length = 100\n",
    "max_title_length = 20\n",
    "label_newnum = len(board_name)\n",
    "\n",
    "new_content_dir = []\n",
    "totalboard_dir = os.path.join(base_dir, totalboard_name)\n",
    "if not os.path.exists(totalboard_dir):\n",
    "    os.makedirs(totalboard_dir)\n",
    "for name_ind in range(len(board_name)):\n",
    "    # New data path\n",
    "    new_content_dir.append(os.path.join(totalboard_dir, board_name[name_ind]+'_content'))\n",
    "    if not os.path.exists(new_content_dir[name_ind]):\n",
    "        os.makedirs(new_content_dir[name_ind])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import jieba\n",
    "\n",
    "# Import dictionary fron jieba and PTT.\n",
    "jieba.set_dictionary('C:/Users/User/Anaconda3/Lib/site-packages/jieba/dict.txt.big')\n",
    "jieba.load_userdict('C:/Users/User/Anaconda3/Lib/site-packages/jieba/userdict.txt')\n",
    "\n",
    "text_content = []\n",
    "text_label = []\n",
    "text_title = []\n",
    "\n",
    "GO_word = '<GO>' #解碼器端的句子起始標識符。\n",
    "EOS_word = '<EOS>' #解碼器端的句子結束標識符。\n",
    "PAD_word = '<PAD>' #補全字符。\n",
    "UNK_word = '<UNK>' #低頻詞或者一些未遇到過的詞等。\n",
    "\n",
    "# Load PTT content and information.\n",
    "#dfs_total = pd.read_csv(os.path.join(totalboard_dir, totalboard_name + '-test.csv'))\n",
    "dfs_total = pd.read_csv(os.path.join(totalboard_dir, totalboard_name + '.csv'))\n",
    "len_total = len(dfs_total)\n",
    "\n",
    "for dfs_index in range(len_total):\n",
    "    read_index = int(dfs_total.iloc[dfs_index, 0])\n",
    "    type_name = str(dfs_total.iloc[dfs_index, 1])\n",
    "    word_length = int(dfs_total.iloc[dfs_index, 5])\n",
    "    title_name = str(dfs_total.iloc[dfs_index, 9])\n",
    "    this_board_name = str(dfs_total.iloc[dfs_index, 10])\n",
    "    \n",
    "    # Add start and end character.\n",
    "    text = GO_word + ' '\n",
    "    with open(os.path.join(new_content_dir[board_dict_index[this_board_name]], str(read_index) + '.csv'),\n",
    "                'r', encoding = 'utf-8-sig') as file:\n",
    "        csvCursor = csv.reader(file)\n",
    "        for rows in csvCursor:\n",
    "            for row in rows:\n",
    "                # Read content and remove empty.\n",
    "                text = text + row\n",
    "        else:\n",
    "            text = text + ' ' + EOS_word\n",
    "    \n",
    "    # Split titles to words by Jieba.\n",
    "    title_name.encode('utf-8-sig')\n",
    "    title_jieba = jieba.cut(title_name, cut_all=False)\n",
    "    title_name = GO_word\n",
    "    word_length = 0\n",
    "    for word in title_jieba:\n",
    "        word_length += 1\n",
    "        title_name = title_name + ' ' + word\n",
    "    else:\n",
    "        title_name = title_name + ' ' + EOS_word\n",
    "\n",
    "    text_content.append(text)\n",
    "    text_label.append(board_dict_index[this_board_name])\n",
    "    text_title.append(title_name)\n",
    "    file.close()\n",
    "\n",
    "# Build vocabulary and convert content to sequence by Keras tool.\n",
    "words_limit = 60000\n",
    "tokenizer = Tokenizer(num_words=words_limit, \n",
    "                      filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n　，。！：；、？﹝﹞「」『』（）｛｝［］【】《》“”‘’＼｜〝〞‵′＋－＊／＝≦≧＿＠＃＄％︿＆～§◎．※ㄧ↔│○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓╱╲╳▁▂▄▅▆▇█▏▎▍▌▋▊▉▁▔', \n",
    "                      lower=False,\n",
    "                      split=\" \")\n",
    "                      #oov_token=UNK_word)\n",
    "tokenizer.fit_on_texts(text_content+text_title)\n",
    "vocab = tokenizer.word_index\n",
    "vocab_counts = tokenizer.word_counts\n",
    "vocab_docs = tokenizer.word_docs\n",
    "\n",
    "print(len(vocab))\n",
    "anti_vocab = {}\n",
    "vocab_keys = list(vocab.keys())\n",
    "for vocab_word in vocab:\n",
    "    vocab_index = vocab[vocab_word]\n",
    "    anti_vocab[vocab_index] = vocab_keys[vocab_index-1]\n",
    "    if vocab_index % 1000 == 0 or vocab_index < 20:\n",
    "        print(vocab[vocab_word], vocab_word, anti_vocab[vocab_index], vocab_counts[vocab_word], vocab_docs[vocab_word])\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_content, text_title, test_size=0.2, random_state=9487)\n",
    "\n",
    "# 将每个词用词典中的数值代替\n",
    "x_train_word_ids = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_word_ids = tokenizer.texts_to_sequences(x_test)\n",
    "y_train_word_ids = tokenizer.texts_to_sequences(y_train)\n",
    "y_test_word_ids = tokenizer.texts_to_sequences(y_test)\n",
    "# 序列模式\n",
    "x_train = pad_sequences(x_train_word_ids, maxlen=max_word_length+2, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test_word_ids, maxlen=max_word_length+2, padding='post', truncating='post')\n",
    "y_train = pad_sequences(y_train_word_ids, maxlen=max_title_length+2, padding='post', truncating='post')\n",
    "y_test = pad_sequences(y_test_word_ids, maxlen=max_title_length+2, padding='post', truncating='post')\n",
    "# 序列長度\n",
    "x_train_size = [len(word_ids) for word_ids in x_train_word_ids]\n",
    "x_test_size = [len(word_ids) for word_ids in x_test_word_ids]\n",
    "y_train_size = [len(word_ids) for word_ids in y_train_word_ids]\n",
    "y_test_size = [len(word_ids) for word_ids in y_test_word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "################\n",
    "# Build seq2seq model\n",
    "################\n",
    "def build_model_in():\n",
    "    ######## model input tensor ########\n",
    "    is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    input_seq_len = tf.placeholder(tf.int32, (None,), name='input_seq_len')\n",
    "    target_seq_len = tf.placeholder(tf.int32, (None,), name='target_seq_len')\n",
    "    limit_target_seq_len = tf.placeholder(tf.int32, (), name='limit_target_seq_len')\n",
    "    return is_training, inputs, targets, input_seq_len, target_seq_len, limit_target_seq_len\n",
    "    \n",
    "def build_model(is_training, inputs, targets, input_seq_len, target_seq_len, limit_target_seq_len):\n",
    "    max_target_seq_len = tf.reduce_max(target_seq_len, name='max_target_seq_len')\n",
    "    validation_batch_size = tf.shape(targets)[0]\n",
    "    start_tokens = tf.tile(tf.constant([vocab[GO_word]], dtype=tf.int32),\n",
    "                               [validation_batch_size], name='start_tokens')\n",
    "    \n",
    "    ######## Embedding for encoder and decoder ########\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size]))\n",
    "    encoder_embed_input = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(embeddings, targets)\n",
    "    \n",
    "    ######## Encoder by LSTM ########\n",
    "    # LSTM cell\n",
    "    def get_lstm_cell(rnn_size, layer):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size, \n",
    "                                            initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "#activity_regularizer=tf.contrib.layers.l2_regularizer(l2_reg_const) \n",
    "        if is_training is not None and lstm_dropout < 1.0:\n",
    "            if layer == 0:\n",
    "                lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "                                                          input_keep_prob=lstm_dropout, \n",
    "                                                          output_keep_prob=lstm_dropout)\n",
    "            else:\n",
    "                lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "                                                          output_keep_prob=lstm_dropout)\n",
    "        return lstm_cell\n",
    "\n",
    "    # Struct encoder by LSTM bi-directional multi-layer\n",
    "    encoder_f_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size, layer) for layer in range(num_layers)])\n",
    "    encoder_b_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size, layer) for layer in range(num_layers)])\n",
    "    encoder_bi_outputs, encoder_bi_state = tf.nn.bidirectional_dynamic_rnn(encoder_f_cell, \n",
    "                                                                           encoder_b_cell, \n",
    "                                                                           encoder_embed_input, \n",
    "                                                                           sequence_length=input_seq_len, \n",
    "                                                                           dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat((encoder_bi_outputs[0], encoder_bi_outputs[1]), 2)\n",
    "    encoder_state_c = [tf.concat((state[0].c, state[1].c), 1) for state in encoder_bi_state]\n",
    "    encoder_state_h = [tf.concat((state[0].h, state[1].h), 1) for state in encoder_bi_state]\n",
    "    encoder_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "    \n",
    "    ######## Decoder by LSTM ########\n",
    "    # Bahdanau attention\n",
    "    def get_decoder_cell(rnn_size, layer, attn_layers, encoder_outputs, input_seq_len):\n",
    "        #attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size*2, encoder_outputs,\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(rnn_size*2, encoder_outputs,\n",
    "                                                                memory_sequence_length=input_seq_len)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(get_lstm_cell(rnn_size*2, layer), \n",
    "                                                       attention_mechanism, \n",
    "                                                       attention_layer_size=attn_layers)\n",
    "        #decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(attn_cell, slot_size)\n",
    "        return attn_cell\n",
    "    \n",
    "    # Struct LSTM cell in Decoder\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size, layer, num_layers, encoder_outputs, input_seq_len) for layer in range(num_layers)])\n",
    "    \n",
    "    # Output fully-connecting layer\n",
    "    output_layer = Dense(vocab_size, \n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    # Struct training decoder\n",
    "    with tf.variable_scope('decode'):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_embed_input,\n",
    "                                                            target_seq_len)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, training_helper,\n",
    "                                                           decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size), \n",
    "                                                           output_layer) \n",
    "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                           impute_finished=True,\n",
    "                                                                           maximum_iterations=max_target_seq_len)\n",
    "    # Struct predicting decoder (Share variable with training)\n",
    "    with tf.variable_scope('decode', reuse=True):\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                     start_tokens, \n",
    "                                                                     vocab[EOS_word])\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, predicting_helper,\n",
    "                                                             decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size), \n",
    "                                                             output_layer)\n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,\n",
    "                                                                            impute_finished=True,\n",
    "                                                                            maximum_iterations=limit_target_seq_len)\n",
    "    ######## Model output tensor ########\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, name='trainings')\n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id, name='predictions')\n",
    "    return training_logits, predicting_logits, max_target_seq_len\n",
    "    \n",
    "def build_optimizer(training_logits, predicting_logits, targets, target_seq_len, max_target_seq_len):\n",
    "    ######## Optimization ########\n",
    "    if is_training is not None:\n",
    "        with tf.name_scope('optimization'):\n",
    "            masks = tf.sequence_mask(target_seq_len, max_target_seq_len, dtype=tf.float32)\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            l2 = tf.Variable(0.0, trainable=False)\n",
    "            # Loss function\n",
    "            cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks, name='sequence_cost')\n",
    "            # Add regularization to Loss\n",
    "            l2 = l2_reg_const * sum( tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() if not ('Bias' in tf_var.name))\n",
    "            #regularization_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "            cost = cost + l2\n",
    "            # Optimizer\n",
    "            learning_rate = tf.train.cosine_decay(lr, global_step, decay_steps, alpha=lr_alpha)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            \n",
    "            # Gradient Clipping\n",
    "            clip_rate = tf.train.cosine_decay(clip_const, global_step, decay_steps, alpha=clip_alpha)\n",
    "            gradients, vriables = zip(*optimizer.compute_gradients(cost))\n",
    "            gradients = [None if gradient is None else tf.clip_by_norm(gradient, clip_rate) for gradient in gradients]\n",
    "            global_step += 1\n",
    "            train_step = optimizer.apply_gradients(zip(gradients, vriables), name='train_step')\n",
    "    return cost, train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1343,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9160,
     "status": "ok",
     "timestamp": 1517421499174,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "-iUlULwfCdGC",
    "outputId": "3a3b3192-bbe8-4534-81d5-53f2e05b9566",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      "epoch1\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  43.304737 ; run time[s]:  81.00932288169861\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.895974 ; run time[s]:  159.58211660385132\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.8805294 ; run time[s]:  237.93737936019897\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.860211 ; run time[s]:  316.9303369522095\n",
      "Iteration  540  ( 49.91 %) Train Loss:  7.1416106 ; run time[s]:  395.0920808315277\n",
      "Iteration  648  ( 59.91 %) Train Loss:  7.6475425 ; run time[s]:  473.487446308136\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.918943 ; run time[s]:  552.2391877174377\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.866954 ; run time[s]:  629.6830413341522\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.531621 ; run time[s]:  707.0747621059418\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.537481 ; run time[s]:  784.6339077949524\n",
      "Train Loss:  6.537481\n",
      "Test loss:  6.620248 ; Std:  0.14864811\n",
      "Target Words: <GO> 白台 是不是 一天到晚 都 盯 著 別人 褲檔 看 <EOS>\n",
      "Response Words: <GO> 我 的 的 <EOS>\n",
      "Save best score!! 6.620248\n",
      "Elapsed time in epoch 1: 856.4929411411285 [s]\n",
      "\n",
      "========\n",
      "epoch2\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.8672075 ; run time[s]:  77.5701756477356\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.5655193 ; run time[s]:  155.22757959365845\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.5092106 ; run time[s]:  232.71553230285645\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.298752 ; run time[s]:  309.26699709892273\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.587907 ; run time[s]:  386.8913505077362\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.570943 ; run time[s]:  463.95819664001465\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.3991475 ; run time[s]:  541.8342053890228\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.6411386 ; run time[s]:  618.5621380805969\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.8252954 ; run time[s]:  696.248619556427\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.7430286 ; run time[s]:  773.9551529884338\n",
      "Train Loss:  6.7430286\n",
      "Test loss:  6.567286 ; Std:  0.13882908\n",
      "Target Words: <GO> 竹科 附近 是否 有 白飯 吃 到 飽 的 小火鍋 <EOS>\n",
      "Response Words: <GO> 有人 的 <EOS>\n",
      "Save best score!! 6.567286\n",
      "Elapsed time in epoch 2: 845.1394066810608 [s]\n",
      "\n",
      "========\n",
      "epoch3\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.38769 ; run time[s]:  77.53909754753113\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.6799145 ; run time[s]:  154.07451915740967\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.6267347 ; run time[s]:  231.74947381019592\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.595525 ; run time[s]:  310.3955044746399\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.495633 ; run time[s]:  389.15730929374695\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.8412194 ; run time[s]:  466.92567110061646\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.58756 ; run time[s]:  545.6094706058502\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.7912254 ; run time[s]:  624.4019012451172\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.8380456 ; run time[s]:  702.8711929321289\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.5597615 ; run time[s]:  781.2362687587738\n",
      "Train Loss:  6.5597615\n",
      "Test loss:  6.6146584 ; Std:  0.14000511\n",
      "Target Words: <GO> 變態 阿嬤 討好 87 歲 男友 逼 孫女 供 他 洩 慾 <EOS>\n",
      "Response Words: <GO> 請問 的 <EOS>\n",
      "Count early stop!! 1\n",
      "Elapsed time in epoch 3: 852.8543355464935 [s]\n",
      "\n",
      "========\n",
      "epoch4\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.9657955 ; run time[s]:  78.68365621566772\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.714466 ; run time[s]:  157.45603561401367\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.658585 ; run time[s]:  235.5216419696808\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.552667 ; run time[s]:  312.77396845817566\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.506706 ; run time[s]:  390.52863073349\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.614361 ; run time[s]:  467.78697299957275\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.2569275 ; run time[s]:  545.2879610061646\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.834173 ; run time[s]:  622.8671567440033\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.690004 ; run time[s]:  700.6709487438202\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.7037745 ; run time[s]:  778.2201023101807\n",
      "Train Loss:  6.7037745\n",
      "Test loss:  6.665535 ; Std:  0.14148127\n",
      "Target Words: <GO> 原來 妳們 是 那種 關係 嗎 <EOS>\n",
      "Response Words: <GO> 關於 的 的 <EOS>\n",
      "Count early stop!! 2\n",
      "Elapsed time in epoch 4: 848.5942506790161 [s]\n",
      "\n",
      "========\n",
      "epoch5\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.5173235 ; run time[s]:  75.32721829414368\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.6319575 ; run time[s]:  151.6039698123932\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.6435513 ; run time[s]:  228.1504421234131\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.7026687 ; run time[s]:  303.5629050731659\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.5611825 ; run time[s]:  379.35337352752686\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.424099 ; run time[s]:  455.4316062927246\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.6465836 ; run time[s]:  533.9481766223907\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.41401 ; run time[s]:  612.8842263221741\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.5996704 ; run time[s]:  692.9254529476166\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.447567 ; run time[s]:  770.9903781414032\n",
      "Train Loss:  6.447567\n",
      "Test loss:  6.609963 ; Std:  0.14175622\n",
      "Target Words: <GO> 台灣 角川 七月 預定 新刊 ISBN 查詢 <EOS>\n",
      "Response Words: <GO> 大家 的 <EOS>\n",
      "Count early stop!! 3\n",
      "Elapsed time in epoch 5: 841.5470631122589 [s]\n",
      "\n",
      "========\n",
      "epoch6\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.682212 ; run time[s]:  75.60295629501343\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.556114 ; run time[s]:  151.42949390411377\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.5377355 ; run time[s]:  227.64609026908875\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.841182 ; run time[s]:  304.2497284412384\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.6095366 ; run time[s]:  380.7811930179596\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.4468503 ; run time[s]:  457.22037959098816\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.874283 ; run time[s]:  533.063971042633\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.601287 ; run time[s]:  609.3607661724091\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.5011725 ; run time[s]:  684.7240724563599\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.5067587 ; run time[s]:  761.0920655727386\n",
      "Train Loss:  6.5067587\n",
      "Test loss:  6.5993595 ; Std:  0.14238653\n",
      "Target Words: <GO> Fate Extra CCC 雷 <EOS>\n",
      "Response Words: <GO> 台北 <EOS>\n",
      "Count early stop!! 4\n",
      "Elapsed time in epoch 6: 831.3418562412262 [s]\n",
      "\n",
      "========\n",
      "epoch7\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  7.383077 ; run time[s]:  76.25669813156128\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.8401 ; run time[s]:  152.09126567840576\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.5873075 ; run time[s]:  229.6785135269165\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.6481757 ; run time[s]:  306.5317893028259\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.469309 ; run time[s]:  382.8366105556488\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.4965405 ; run time[s]:  459.2737798690796\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.734656 ; run time[s]:  535.7871510982513\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.461854 ; run time[s]:  612.0879580974579\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.392685 ; run time[s]:  688.2132933139801\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.645651 ; run time[s]:  764.1721992492676\n",
      "Train Loss:  6.645651\n",
      "Test loss:  6.597268 ; Std:  0.13269798\n",
      "Target Words: <GO> 我該 怎麼辦 <EOS>\n",
      "Response Words: <GO> 台 的 <EOS>\n",
      "Count early stop!! 5\n",
      "Elapsed time in epoch 7: 834.383898973465 [s]\n",
      "\n",
      "========\n",
      "epoch8\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.309145 ; run time[s]:  76.40408158302307\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.5232263 ; run time[s]:  152.94954228401184\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.6482067 ; run time[s]:  229.55113983154297\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.4954805 ; run time[s]:  305.82192277908325\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.3429995 ; run time[s]:  382.1598918437958\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.543011 ; run time[s]:  458.52087783813477\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.7562423 ; run time[s]:  535.2157692909241\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.6418233 ; run time[s]:  611.507562160492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  972  ( 89.91 %) Train Loss:  6.5180817 ; run time[s]:  688.6385669708252\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.625304 ; run time[s]:  765.1890280246735\n",
      "Train Loss:  6.625304\n",
      "Test loss:  6.6070476 ; Std:  0.12851271\n",
      "Target Words: <GO> 南科 包子 TFT 製程 設備 工程師 <EOS>\n",
      "Response Words: <GO> 關於 的 <EOS>\n",
      "Count early stop!! 6\n",
      "Elapsed time in epoch 8: 835.3496370315552 [s]\n",
      "\n",
      "========\n",
      "epoch9\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  7.2468357 ; run time[s]:  76.20155620574951\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.708917 ; run time[s]:  152.6437737941742\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.7817893 ; run time[s]:  229.07994484901428\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.6239 ; run time[s]:  305.4138481616974\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.788093 ; run time[s]:  382.1518213748932\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.4652553 ; run time[s]:  458.6451930999756\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.850077 ; run time[s]:  535.2227439880371\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.488924 ; run time[s]:  611.5526320934296\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.3634267 ; run time[s]:  688.0399255752563\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.889815 ; run time[s]:  763.816338300705\n",
      "Train Loss:  6.889815\n",
      "Test loss:  6.5784206 ; Std:  0.13419458\n",
      "Target Words: <GO> 七 產業 將 鬆綁 陸資 控制力 <EOS>\n",
      "Response Words: <GO> 請問 的 <EOS>\n",
      "Count early stop!! 7\n",
      "Elapsed time in epoch 9: 834.0401263237 [s]\n",
      "\n",
      "========\n",
      "epoch10\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.3977795 ; run time[s]:  76.46123170852661\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.361187 ; run time[s]:  153.32454109191895\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.530751 ; run time[s]:  229.8279230594635\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.7272124 ; run time[s]:  306.5989935398102\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.7644296 ; run time[s]:  382.6461637020111\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.5874443 ; run time[s]:  459.3269715309143\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.551265 ; run time[s]:  535.617742061615\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.799726 ; run time[s]:  611.8784472942352\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.469088 ; run time[s]:  688.3527181148529\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.600402 ; run time[s]:  765.1057398319244\n",
      "Train Loss:  6.600402\n",
      "Test loss:  6.5761294 ; Std:  0.13249424\n",
      "Target Words: <GO> 南科 包子 TFT 製程 設備 工程師 <EOS>\n",
      "Response Words: <GO> 大家 的 <EOS>\n",
      "Count early stop!! 8\n",
      "Elapsed time in epoch 10: 835.4387748241425 [s]\n",
      "\n",
      "========\n",
      "epoch11\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.549356 ; run time[s]:  76.74999594688416\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.709103 ; run time[s]:  153.92613697052002\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.678633 ; run time[s]:  230.7694127559662\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.729609 ; run time[s]:  307.3489646911621\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.477281 ; run time[s]:  383.4963915348053\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.7253375 ; run time[s]:  460.1661972999573\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.7133527 ; run time[s]:  536.741738319397\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.549432 ; run time[s]:  613.2310814857483\n",
      "Iteration  972  ( 89.91 %) Train Loss:  7.042115 ; run time[s]:  689.6733179092407\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.9905853 ; run time[s]:  766.7361662387848\n",
      "Train Loss:  6.9905853\n",
      "Test loss:  7.1607647 ; Std:  0.14770561\n",
      "Target Words: <GO> 裁員 頻繁 的 公司 怎麼 調整 <EOS>\n",
      "Response Words: <GO> 的 的 <EOS>\n",
      "Count early stop!! 9\n",
      "Elapsed time in epoch 11: 836.9388434886932 [s]\n",
      "\n",
      "========\n",
      "epoch12\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.5884514 ; run time[s]:  76.98764491081238\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.557965 ; run time[s]:  153.12503480911255\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.7293677 ; run time[s]:  229.47397756576538\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.3631015 ; run time[s]:  306.0334665775299\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.549991 ; run time[s]:  382.61303544044495\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.6400666 ; run time[s]:  458.6361265182495\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.8547196 ; run time[s]:  534.51083111763\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.861786 ; run time[s]:  611.5185306072235\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.506601 ; run time[s]:  688.1632513999939\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.707591 ; run time[s]:  764.608460187912\n",
      "Train Loss:  6.707591\n",
      "Test loss:  6.565671 ; Std:  0.13202356\n",
      "Target Words: <GO> 如果 女生 先 動手 男生 會 還手 嗎 <EOS>\n",
      "Response Words: <GO> 請問 的 <EOS>\n",
      "Save best score!! 6.565671\n",
      "Elapsed time in epoch 12: 834.6928520202637 [s]\n",
      "\n",
      "========\n",
      "epoch13\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.5374002 ; run time[s]:  75.93081855773926\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.778656 ; run time[s]:  152.36698484420776\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.774201 ; run time[s]:  228.54551672935486\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.745019 ; run time[s]:  305.32760310173035\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.3886337 ; run time[s]:  381.68058490753174\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.497131 ; run time[s]:  458.46367740631104\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.464265 ; run time[s]:  534.5148115158081\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.4815583 ; run time[s]:  612.386789560318\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.6429596 ; run time[s]:  688.8169527053833\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.7545185 ; run time[s]:  765.4335942268372\n",
      "Train Loss:  6.7545185\n",
      "Test loss:  6.5567255 ; Std:  0.13614413\n",
      "Target Words: <GO> <EOS>\n",
      "Response Words: <GO> 請問 的 的 <EOS>\n",
      "Save best score!! 6.5567255\n",
      "Elapsed time in epoch 13: 835.5922477245331 [s]\n",
      "\n",
      "========\n",
      "epoch14\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.799209 ; run time[s]:  76.25167036056519\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.539689 ; run time[s]:  152.99464392662048\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.5121145 ; run time[s]:  229.31449246406555\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.780467 ; run time[s]:  305.59525895118713\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.6856837 ; run time[s]:  382.6871681213379\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.5260534 ; run time[s]:  458.6661102771759\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.6535535 ; run time[s]:  535.2897832393646\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.645225 ; run time[s]:  611.7710654735565\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.4787416 ; run time[s]:  687.6647808551788\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.403619 ; run time[s]:  764.045823097229\n",
      "Train Loss:  6.403619\n",
      "Test loss:  6.5719438 ; Std:  0.14121926\n",
      "Target Words: <GO> 紅豬 想 傳達 的 是 什麼 <EOS>\n",
      "Response Words: <GO> 請問 的 <EOS>\n",
      "Count early stop!! 1\n",
      "Elapsed time in epoch 14: 834.4701118469238 [s]\n",
      "\n",
      "========\n",
      "epoch15\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.654429 ; run time[s]:  77.32655477523804\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.526977 ; run time[s]:  154.42047357559204\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.4115276 ; run time[s]:  230.7553813457489\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.586386 ; run time[s]:  307.7480490207672\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.8223467 ; run time[s]:  384.4980456829071\n",
      "Iteration  648  ( 59.91 %) Train Loss:  18.173271 ; run time[s]:  460.47599387168884\n",
      "Iteration  756  ( 69.91 %) Train Loss:  12.618208 ; run time[s]:  537.2872292995453\n",
      "Iteration  864  ( 79.91 %) Train Loss:  20.259624 ; run time[s]:  614.3620846271515\n",
      "Iteration  972  ( 89.91 %) Train Loss:  9.242814 ; run time[s]:  690.8213119506836\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  7.4197173 ; run time[s]:  767.2956013679504\n",
      "Train Loss:  7.4197173\n",
      "Test loss:  7.539871 ; Std:  0.14191864\n",
      "Target Words: <GO> 獸性大發 男子 性侵 遭 踢 斷腿 <EOS>\n",
      "Response Words: <GO> 有人 男友 男友 女生 女生 的 的 的 的 的 的 <EOS>\n",
      "Count early stop!! 2\n",
      "Elapsed time in epoch 15: 837.6286282539368 [s]\n",
      "\n",
      "========\n",
      "epoch16\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  7.123718 ; run time[s]:  77.56516647338867\n",
      "Iteration  216  ( 19.91 %) Train Loss:  7.3076677 ; run time[s]:  154.1557433605194\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.7221446 ; run time[s]:  230.8536355495453\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.637418 ; run time[s]:  307.2857871055603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  540  ( 49.91 %) Train Loss:  6.6754956 ; run time[s]:  384.0308048725128\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.3921556 ; run time[s]:  460.73768973350525\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.5141697 ; run time[s]:  536.9051430225372\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.7466755 ; run time[s]:  613.0304877758026\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.4498606 ; run time[s]:  690.0743005275726\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.592837 ; run time[s]:  766.7611410617828\n",
      "Train Loss:  6.592837\n",
      "Test loss:  6.475089 ; Std:  0.15730193\n",
      "Target Words: <GO> 注意 我 甜 竟 幫 這個 男 的 慶生 <EOS>\n",
      "Response Words: <GO> 請問 喜歡 我 我 我 女生 女生 女生 片 片 喜歡 女生 女生 女生 女生 女生 電影 的 電影 的 的\n",
      "Save best score!! 6.475089\n",
      "Elapsed time in epoch 16: 837.1032180786133 [s]\n",
      "\n",
      "========\n",
      "epoch17\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.1966934 ; run time[s]:  76.61464977264404\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.8668995 ; run time[s]:  153.10093998908997\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.7407303 ; run time[s]:  229.305499792099\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.659519 ; run time[s]:  306.1507487297058\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.485898 ; run time[s]:  383.6938714981079\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.542938 ; run time[s]:  460.20925784111023\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.190555 ; run time[s]:  537.0474843978882\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.493235 ; run time[s]:  613.5237712860107\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.6285744 ; run time[s]:  690.3810606002808\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.316491 ; run time[s]:  766.7550611495972\n",
      "Train Loss:  6.316491\n",
      "Test loss:  6.4401345 ; Std:  0.1559792\n",
      "Target Words: <GO> 七 產業 將 鬆綁 陸資 控制力 <EOS>\n",
      "Response Words: <GO> 請問 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是 是不是\n",
      "Save best score!! 6.4401345\n",
      "Elapsed time in epoch 17: 837.1702868938446 [s]\n",
      "\n",
      "========\n",
      "epoch18\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.581421 ; run time[s]:  76.31484818458557\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.184425 ; run time[s]:  153.31049275398254\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.348242 ; run time[s]:  229.60329127311707\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.4581804 ; run time[s]:  305.9983448982239\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.6119385 ; run time[s]:  381.68553829193115\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.7191324 ; run time[s]:  458.16383934020996\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.590848 ; run time[s]:  534.7414133548737\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.6609898 ; run time[s]:  610.9870686531067\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.5868573 ; run time[s]:  687.1093971729279\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.4024444 ; run time[s]:  763.228716135025\n",
      "Train Loss:  6.4024444\n",
      "Test loss:  6.4228263 ; Std:  0.1574163\n",
      "Target Words: <GO> 1976 樂團 桃園 場 今晚 <EOS>\n",
      "Response Words: <GO> 你們 大 大 大 感覺 大 感覺 感覺 大 大 大 感覺 大 感覺 大 感覺 感覺 感覺 感覺 感覺 感覺\n",
      "Save best score!! 6.4228263\n",
      "Elapsed time in epoch 18: 833.5005435943604 [s]\n",
      "\n",
      "========\n",
      "epoch19\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.642023 ; run time[s]:  76.9405107498169\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.4957533 ; run time[s]:  153.5631902217865\n",
      "Iteration  324  ( 29.91 %) Train Loss:  6.5011826 ; run time[s]:  229.70760679244995\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.298032 ; run time[s]:  309.06252217292786\n",
      "Iteration  540  ( 49.91 %) Train Loss:  6.350755 ; run time[s]:  389.016028881073\n",
      "Iteration  648  ( 59.91 %) Train Loss:  6.4566827 ; run time[s]:  467.8435423374176\n",
      "Iteration  756  ( 69.91 %) Train Loss:  6.2892466 ; run time[s]:  547.4511291980743\n",
      "Iteration  864  ( 79.91 %) Train Loss:  6.287498 ; run time[s]:  625.8765859603882\n",
      "Iteration  972  ( 89.91 %) Train Loss:  6.3972063 ; run time[s]:  702.2074825763702\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  6.5789385 ; run time[s]:  778.4210712909698\n",
      "Train Loss:  6.5789385\n",
      "Test loss:  6.4658666 ; Std:  0.15502705\n",
      "Target Words: <GO> 有 美國 對於 在 矽谷 談 薪水 有 幫助 嗎 <EOS>\n",
      "Response Words: <GO> 台積電 晚上 對方 接受 對方 接受 對方 接受 對方 時間 對方 對方 篇 2 怎麼 是 是 嗎 的 嗎 嗎\n",
      "Count early stop!! 1\n",
      "Elapsed time in epoch 19: 848.7551162242889 [s]\n",
      "\n",
      "========\n",
      "epoch20\n",
      "========\n",
      "Iteration  108  ( 9.91 %) Train Loss:  6.4853396 ; run time[s]:  76.04012727737427\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.364196 ; run time[s]:  153.1119794845581\n",
      "Iteration  324  ( 29.91 %) Train Loss:  8.660988 ; run time[s]:  229.78476548194885\n",
      "Iteration  432  ( 39.91 %) Train Loss:  6.449725 ; run time[s]:  306.4916477203369\n",
      "Iteration  540  ( 49.91 %) Train Loss:  21.429388 ; run time[s]:  383.21959829330444\n",
      "Iteration  648  ( 59.91 %) Train Loss:  19.652332 ; run time[s]:  460.34759521484375\n",
      "Iteration  756  ( 69.91 %) Train Loss:  26.74988 ; run time[s]:  536.706563949585\n",
      "Iteration  864  ( 79.91 %) Train Loss:  18.124653 ; run time[s]:  613.2470071315765\n",
      "Iteration  972  ( 89.91 %) Train Loss:  14.501584 ; run time[s]:  689.3813672065735\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  12.360367 ; run time[s]:  766.0241193771362\n",
      "Train Loss:  12.360367\n",
      "Test loss:  12.382175 ; Std:  0.14974986\n",
      "Target Words: <GO> 女生 根本 不用 追 <EOS>\n",
      "Response Words: <GO> 我該 晚上 約 生日 約 小 小 妳 小 後 後 小 後 約 小 小 後 小 後 小 <EOS>\n",
      "Count early stop!! 2\n",
      "Elapsed time in epoch 20: 836.373176574707 [s]\n",
      "\n",
      "Elapsed time in total: 16816.0988137722\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "################\n",
    "# Set parameter\n",
    "################\n",
    "patience = 0\n",
    "num_epoch = 20\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "vocab_size = words_limit\n",
    "limit_generation_len = max_title_length + 2\n",
    "num_instances = len(y_train)\n",
    "iter_per_epoch = int(num_instances / batch_size)\n",
    "test_num_instances = len(y_test)\n",
    "test_iter_per_epoch = int(test_num_instances / test_batch_size)\n",
    "\n",
    "rnn_size = 150\n",
    "num_layers = 3\n",
    "slot_size = 172\n",
    "embedding_size = 300\n",
    "\n",
    "lstm_dropout = 0.5\n",
    "l2_reg_const = 0.001\n",
    "\n",
    "lr = 0.01\n",
    "lr_alpha = 1e-3\n",
    "decay_steps = 10 * num_instances / batch_size\n",
    "\n",
    "clip_const = 50.0\n",
    "clip_alpha = 5.0\n",
    "\n",
    "total_start_t = time.time()\n",
    "build_new_model = 1\n",
    "if build_new_model:\n",
    "    str_epoch = 0\n",
    "    model_name = 'PTT_Encoder-Decoder_model_fixed_' + str(total_start_t)\n",
    "else:\n",
    "    str_epoch = 30\n",
    "    model_name = 'PTT_Encoder-Decoder_model_fixed_1522421878.8546176'\n",
    "\n",
    "################\n",
    "# Prepare to run session\n",
    "################\n",
    "iter_pct10 = int(iter_per_epoch / 10)\n",
    "if (num_instances % batch_size) > 0:\n",
    "    iter_per_epoch += 1\n",
    "batch_cutoff = [0]\n",
    "for i in range(iter_per_epoch - 1):\n",
    "    batch_cutoff.append(batch_size * (i+1))\n",
    "batch_cutoff.append(num_instances)\n",
    "\n",
    "best_validation_loss = 0.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "mdl_dir = os.path.join(base_dir, 'model')\n",
    "if not os.path.exists(mdl_dir):\n",
    "    os.makedirs(mdl_dir)\n",
    "model_dir = os.path.join(mdl_dir, model_name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "################\n",
    "# Run model session\n",
    "################\n",
    "train_graph = tf.Graph()\n",
    "if build_new_model:\n",
    "    ######## Build model parameters ########\n",
    "    with train_graph.as_default():\n",
    "        # Build input parameters\n",
    "        is_training, inputs, targets, input_seq_len, target_seq_len, limit_target_seq_len = build_model_in()\n",
    "        # Build model and  weight parameters\n",
    "        training_logits, predicting_logits, max_target_seq_len = build_model(is_training, inputs, targets, \n",
    "                                                                             input_seq_len, target_seq_len, \n",
    "                                                                             limit_target_seq_len)\n",
    "        # Build optimizer\n",
    "        cost, train_step = build_optimizer(training_logits, predicting_logits,\n",
    "                                           targets, target_seq_len, max_target_seq_len)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    if build_new_model:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        best_epoch = 1\n",
    "    else:\n",
    "        ######## Load model parameters ########\n",
    "        # Load model and weight parameters\n",
    "        best_epoch = str_epoch\n",
    "        load_model_dir = os.path.join(model_dir, 'model-{}'.format(str_epoch))\n",
    "        loader = tf.train.import_meta_graph(load_model_dir + '.meta')\n",
    "        loader.restore(sess, load_model_dir)\n",
    "        # Load named input parameters\n",
    "        is_training = train_graph.get_tensor_by_name('is_training:0')\n",
    "        inputs = train_graph.get_tensor_by_name('inputs:0')\n",
    "        targets = train_graph.get_tensor_by_name('targets:0')\n",
    "        input_seq_len = train_graph.get_tensor_by_name('input_seq_len:0')\n",
    "        target_seq_len = train_graph.get_tensor_by_name('target_seq_len:0')\n",
    "        limit_target_seq_len = train_graph.get_tensor_by_name('limit_target_seq_len:0')\n",
    "        # Load named output and optimizer parameters\n",
    "        training_logits = train_graph.get_tensor_by_name('trainings:0')\n",
    "        predicting_logits = train_graph.get_tensor_by_name('predictions:0')\n",
    "        cost = train_graph.get_tensor_by_name('optimization/sequence_cost/truediv:0')\n",
    "        train_step = train_graph.get_operation_by_name('optimization/gradients_1')\n",
    "        \n",
    "    ######## Run model epoch ########\n",
    "    for e in range(str_epoch,str_epoch+num_epoch):\n",
    "        rand_idxs = np.random.permutation(num_instances)\n",
    "        print('\\n========')\n",
    "        print('epoch' + str(e+1))\n",
    "        print('========')\n",
    "        start_t = time.time()\n",
    "        \n",
    "        ######## Get random input batch and training model. ########\n",
    "        for i in range(iter_per_epoch):\n",
    "            X_batch = []\n",
    "            Y_batch = []\n",
    "            X_size = []\n",
    "            Y_size = []\n",
    "            for n in range(batch_cutoff[i],batch_cutoff[i+1]):\n",
    "                X_batch.append(x_train[rand_idxs[n]])\n",
    "                Y_batch.append(y_train[rand_idxs[n]])\n",
    "                X_size.append(x_train_size[rand_idxs[n]])\n",
    "                Y_size.append(y_train_size[rand_idxs[n]])\n",
    "            max_X_size = max(X_size)\n",
    "            max_Y_size = max(Y_size)\n",
    "            X_batch = np.array(X_batch)\n",
    "            Y_batch = np.array(Y_batch)\n",
    "            X_size = np.array(X_size)\n",
    "            Y_size = np.array(Y_size)\n",
    "            \n",
    "            _, loss = sess.run( [train_step, cost],\n",
    "                                {is_training: True,\n",
    "                                 inputs: X_batch[:,:max_X_size], targets: Y_batch[:,:max_Y_size],\n",
    "                                 input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                                 limit_target_seq_len: limit_generation_len})\n",
    "            \n",
    "            if i % iter_pct10 == iter_pct10 - 1:\n",
    "                print('Iteration ',i+1,' (',round(i*100/iter_per_epoch,2),'%) Train Loss: ',\n",
    "                      loss,'; run time[s]: ', time.time() - start_t,end='\\n')\n",
    "        \n",
    "        print('Train Loss: ',loss)\n",
    "        \n",
    "        ######## Get validation input batch and validating model. ########\n",
    "        test_loss = []\n",
    "        for i in range(test_iter_per_epoch):\n",
    "            i_str = i * test_batch_size\n",
    "            i_end = i_str + test_batch_size\n",
    "            \n",
    "            X_batch = np.array(x_test[i_str:i_end])\n",
    "            Y_batch = np.array(y_test[i_str:i_end])\n",
    "            X_size = np.array(x_test_size[i_str:i_end])\n",
    "            Y_size = np.array(y_test_size[i_str:i_end])\n",
    "            max_X_size = max(X_size)\n",
    "            max_Y_size = max(Y_size)\n",
    "            y_loss = sess.run( cost,{inputs: X_batch[:,:max_X_size], targets: Y_batch[:,:max_Y_size],\n",
    "                                     input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                                     limit_target_seq_len: limit_generation_len})\n",
    "            test_loss.append(y_loss)\n",
    "        else:\n",
    "            test_loss = np.asarray(test_loss)\n",
    "            validation_loss = test_loss.mean()\n",
    "            validation_loss_std = test_loss.std()\n",
    "        print('Test loss: ',validation_loss,'; Std: ',validation_loss_std)\n",
    "        \n",
    "        ######## Generate title form testing content and validating model. ########\n",
    "        answer_logits = sess.run( predicting_logits, \n",
    "                                {inputs: X_batch[:,:max_X_size], targets: Y_batch[:,:max_Y_size], \n",
    "                                 input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                                 limit_target_seq_len: limit_generation_len})\n",
    "        output_index = random.randint(0,test_batch_size-1)\n",
    "        print('Target Words: {}'.format(\" \".join([anti_vocab[i] for i in Y_batch[output_index] if i != 0])))\n",
    "        print('Response Words: {}'.format(\" \".join([anti_vocab[i] for i in answer_logits[output_index] if i != 0])))\n",
    "        \n",
    "        ######## Save model weight parameters and information. ########\n",
    "        if validation_loss < best_validation_loss or e == 0:\n",
    "            best_validation_loss = validation_loss\n",
    "            best_epoch = e\n",
    "            early_stop_counter = 0\n",
    "            print('Save best score!! '+str(best_validation_loss))\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print('Count early stop!! '+str(early_stop_counter))\n",
    "\n",
    "        print('Elapsed time in epoch ' + str(e+1) + ': ' + str(time.time() - start_t) + ' [s]')\n",
    "\n",
    "        model_path = os.path.join(model_dir, 'model-%d' %(e+1))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, model_path)\n",
    "        \n",
    "        if patience != 0 and early_stop_counter >= patience:\n",
    "            print('\\n#######')\n",
    "            print('Best model')\n",
    "            print('#######')\n",
    "            print('Stop by early stopping')\n",
    "            print('Best score: ', best_validation_loss, 'Beat model: ', best_epoch)\n",
    "            break\n",
    "\n",
    "print('\\nElapsed time in total: ' + str(time.time() - total_start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch:  20\n",
      "INFO:tensorflow:Restoring parameters from C:/Users/User/Raw data/PTT\\model\\PTT_Encoder-Decoder_model_fixed_1522514424.640014\\model-20\n",
      "\n",
      "========\n",
      "Test index:  12779\n",
      "========\n",
      "Test input Words: <GO> 連假 好 無聊 聊聊 啊 因為 隔天 要 工作 所以 沒有 出去玩 今晚 想 去 看 潘恩 好 喜歡 休 傑克 曼 啊啊啊 想 揪 台 中 的 朋友 陪 我 一起 看 場次 如下 因為 我 有 優惠 票 一個 人 180 元 怕 你們 覺得 我 詐騙 所以 到 現場 跟 我 拿票 以後 再 跟 我 錢 最多 6 位 9 30 截止 想 跟 我們 一起 看 電影 的 請 私訊 我 喔 LINE <EOS>\n",
      "Target Words: <GO> 台 中 老虎 城 一起 看 潘恩 <EOS>\n",
      "Response Words: <GO> 今天 2 2 片 去 去 不 不 去 片 去 去 電影 電影 電影 電影 嗎 嗎 嗎 的 <EOS>\n",
      "\n",
      "========\n",
      "Test index:  7984\n",
      "========\n",
      "Test input Words: <GO> 大家 好 小弟 是 快要 畢業 的 115 碩 有 兩個 研替 offer 想 請問 前輩 們 公司 啟 碁 機器 學習 工程師 矽品 PIE 地點 竹科 中科 工時 8 8 00 住宿 租屋 租屋 薪資 N 14 獎金 N 16 N 為 GG 碩士 新人 價 啟 碁 的 職缺 與 我學 的 比較 相關 矽品 的 職缺 感覺 就是 進去 全部 重學 想 請問 前輩 們 哪個 職缺 或 公司 比較 有 前景 謝謝 <EOS>\n",
      "Target Words: <GO> 研替 offer 請益 <EOS>\n",
      "Response Words: <GO> 第 2 不 不 電影 電影 了 了 嗎 的 的 <EOS>\n",
      "\n",
      "========\n",
      "Test index:  5212\n",
      "========\n",
      "Test input Words: <GO> 想 請問 各位 因為 之前 有 投 履歷 跟 在 台積 電網 站 登錄 履歷 應徵 技術員 因為過 了 1 2 個 月 沒 消息 就 放棄 可是 今天 收到 一個 簡 訊 上面 積電 龍潭 科學園區 封裝 廠 技術員 徵才 說要 證件 穿著 襪子 有 意願 就 簡訊 回覆 然後 上台 時 卻 發現 我當 初 登錄 應徵 的 帳號 已經 被 砍 了 想 請問 這 簡訊 是否是 真的 另外 請問 封裝 廠 技術員 他 的 待遇 是 抱歉 落落 長 希望 能 幫忙 解答 先 謝謝 了 Sent from my Android <EOS>\n",
      "Target Words: <GO> 請問 台積電 應徵 技術員 問題 <EOS>\n",
      "Response Words: <GO> 今天 2 去 去 不 不 去 去 不 不 電影 電影 不 電影 了 嗎 嗎 嗎 的 <EOS>\n",
      "\n",
      "========\n",
      "Test index:  6480\n",
      "========\n",
      "Test input Words: <GO> 以下 截取 自 陳 玉勳 導演 粉絲 專頁 看 了 行動代號 孫中山 突然 覺得 應該 來 拍 一部 電影 幾位 外勞 被 黑心 台灣 老闆 壓榨 不得已 計畫 一個 卻 不 小心 綁 到 了 總統 結果 民眾 喜極而泣 大家 集資 一筆 龐大 費用 送 他們 回家 鄉 並且 請 他們 把 貨 帶走 不要 回來 熱帶魚 2 想要 拍 這個 題材 的 人 請 跟 我 買 版權 謝謝 好像 挺 有 看頭 的 <EOS>\n",
      "Target Words: <GO> 熱帶魚 2 <EOS>\n",
      "Response Words: <GO> 我該 晚上 約 約 小 小 後 後 後 後 小 後 後 後 後 後 小 後 後 小 小\n",
      "\n",
      "========\n",
      "Test index:  4630\n",
      "========\n",
      "Test input Words: <GO> 年紀 越來越 大 生活圈 卻 越來越 小 身邊 的 姊妹 大都 已 結婚 平常 也 沒 啥 時間 可以 聚聚 因此 想來 徵個 可以 一起 分享 生活 的 好友 男女 不拘 如果 可以 一起 運動 聊天 也 很 不錯 關於 我 1986 年生 是 個 厚片 女孩 最近 正 努力 運動 減肥 中 在 台南 上班 喜歡 聽 音樂 看 電影 但 卻 很 難 找到 伴 可以 同行 非正妹 如想 找 正妹 的 可以 不用 看 了 剛 結束 一段 感情 不久 發現 除了 他 生活圈 小 幾乎 沒有 甚麼 朋友 平時 下班 後 會 去 運動 快 走 然後 在家 當個 關於 你 男女 不拘 但 希望 年紀 不要 差太多 正負 6 歲 內 皆 可 其他 的 就 來信 聊 囉 <EOS>\n",
      "Target Words: <GO> 一起 分享 生活 的 好友 <EOS>\n",
      "Response Words: <GO> 晚上 後 小 後 後 後 後 小 後 後 後 小 小 後 後 小 小 後 小 後 後\n",
      "\n",
      "========\n",
      "Test index:  12822\n",
      "========\n",
      "Test input Words: <GO> 嗯 這 四個 女主角 應該 都 50 歲 以上 了 就算 影集 的 時代 都還 30 左右 的 時候 除了 演 夏綠蒂 的 Davis 算是 美女 其他 三個 只能 算好 來 尤其 最紅 的 那個 根本就是 馬 臉 現在 50 看 嗎 哀哀 我 看 最近 的 照片 連 Davis 這種 娃娃臉 美女 都 出現 的 其他 三個 應該 更 不用說 剛剛 才 看 完 Davis 有演 的 2009 裡面 還是 很 迷人 過 7 年 就 老 很多 的 感覺 不過 也 可能 是 演 電影 會 化妝 和 後 製 xd <EOS>\n",
      "Target Words: <GO> 慾望 城市 回來 了 女主角 點頭 答應 接拍 第 3 <EOS>\n",
      "Response Words: <GO> 徵求 後 後 後 小 後 <EOS>\n",
      "\n",
      "========\n",
      "Test index:  7718\n",
      "========\n",
      "Test input Words: <GO> 啊 突然 想到 我國 中 被 霸凌 的 其中 一個 要點 是不是 就是 這個 啊 我 從國 一下 開始 上下學 都 搭 校車 然後 我 同車 坐 我 旁邊 那個 跟 我 同年級 有次 我 看 他 在 用 手機 看 影片 問他 在 看 什麼 他 直接 拿給 我 看 影片 是 家庭教師 姐姐 當下 第一次 看到 那種 片 的 我 很 興奮 的 叫 他 傳給 我 隔天 早上 去 學校 我 居然 直接 在 教室 看 還跟 別人 炫耀 Sent from JPTT on my Sony E6853 <EOS>\n",
      "Target Words: <GO> 大家 國 高中 關於 acg 的 黑 歷史 <EOS>\n",
      "Response Words: <GO> 臺北 2 工程師 片 片 去 片 去 不 電影 了 嗎 嗎 嗎 的 <EOS>\n",
      "\n",
      "========\n",
      "Test index:  5311\n",
      "========\n",
      "Test input Words: <GO> 本季 有 興趣 的 不 多 只有 六花 勇者 和 戰姬 輕鬆 有趣 的 有 下流 梗 不 存在 的 世界 目前 已經 看 了 濃濃 日 和 但 總覺 得 hp 不 起來 想 多 看點 難民 番 想 多 看 一點點 又 怕 踩到 致鬱系 作品 不 知道 是否 有 其他 難民 番 可 推薦 另外 有沒有 人 知道 美國 的 費城 有沒有 啥 跟 動漫 有關 的 店 可 逛 啊 要 去 那邊 住 一陣子 如果 完全 沒 東西 可以 補 hp 人生 就 變成 黑白 了 <EOS>\n",
      "Target Words: <GO> 本季 的 難民 番 <EOS>\n",
      "Response Words: <GO> 我該 晚上 約 生日 生日 約 小 後 後 <EOS>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "################################\n",
    "#Generate title form testing content and validating model. \n",
    "################################\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "model_name = 'PTT_Encoder-Decoder_model_fixed_1522514424.640014'\n",
    "epoch = 20\n",
    "model_path = os.path.join(base_dir, 'model', model_name, 'model-{}'.format(epoch))\n",
    "print('Test epoch: ', epoch)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(model_path + '.meta')\n",
    "    loader.restore(sess, model_path)\n",
    "    # Named parameters\n",
    "    #is_training = loaded_graph.get_tensor_by_name('is_training:0')\n",
    "    inputs = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    targets = loaded_graph.get_tensor_by_name('targets:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    input_seq_len = loaded_graph.get_tensor_by_name('input_seq_len:0')\n",
    "    target_seq_len = loaded_graph.get_tensor_by_name('target_seq_len:0')\n",
    "    limit_target_seq_len = loaded_graph.get_tensor_by_name('limit_target_seq_len:0')\n",
    "    \n",
    "    for _ in range(8):\n",
    "        i_str = random.randint(0,test_num_instances-1)\n",
    "        i_end = i_str + 1\n",
    "\n",
    "        X_batch = np.tile(x_test[i_str], (test_batch_size, 1))\n",
    "        Y_batch = np.tile(y_test[i_str], (test_batch_size, 1))\n",
    "        X_size = np.tile(x_test_size[i_str], (test_batch_size))\n",
    "        Y_size = np.tile(y_test_size[i_str], (test_batch_size))\n",
    "        max_X_size = x_test_size[i_str]\n",
    "        max_Y_size = y_test_size[i_str]\n",
    "        answer_logits = sess.run( logits, {inputs: X_batch, targets: Y_batch, \n",
    "                                             input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                                             limit_target_seq_len: limit_generation_len})\n",
    "\n",
    "        output_index = 0 #random.randint(0,test_batch_size-1)\n",
    "        print('\\n========')\n",
    "        print('Test index: ', i_str + output_index)\n",
    "        print('========')\n",
    "        print('Test input Words: {}'.format(\" \".join([anti_vocab[i] for i in X_batch[output_index] if i != 0])))\n",
    "        print('Target Words: {}'.format(\" \".join([anti_vocab[i] for i in Y_batch[output_index] if i != 0])))\n",
    "        print('Response Words: {}'.format(\" \".join([anti_vocab[i] for i in answer_logits[output_index] if i != 0])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "multi-PTT text classification model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
