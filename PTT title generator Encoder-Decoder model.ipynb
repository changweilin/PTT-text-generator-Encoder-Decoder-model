{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "D72meokFwX7J",
    "outputId": "420d76a4-edf0-492d-9ee9-07b6933c685b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from C:\\Users\\User\\Anaconda3\\Lib\\site-packages\\jieba\\dict.txt.big ...\n",
      "Dumping model to file cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.ufa6ae29b0cbce8b45e006c7fa30eaaf8.cache\n",
      "Loading model cost 1.281 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199446\n",
      "1 的 的 479528 108422\n",
      "2 我 我 187622 60294\n",
      "3 <GO> <GO> 172800 172800\n",
      "4 <EOS> <EOS> 172800 172800\n",
      "5 是 是 157302 68972\n",
      "6 了 了 129931 60543\n",
      "7 有 有 108041 59614\n",
      "8 在 在 97066 54361\n",
      "9 也 也 86587 48759\n",
      "10 都 都 75404 45560\n",
      "11 就 就 71400 42410\n",
      "12 不 不 66238 42797\n",
      "13 很 很 54725 34820\n",
      "14 會 會 54210 36201\n",
      "15 他 他 49560 20170\n",
      "16 跟 跟 48539 31439\n",
      "17 看 看 48476 30452\n",
      "18 你 你 48168 21707\n",
      "19 想 想 47770 34168\n",
      "1000 跳 跳 1246 1022\n",
      "2000 整合 整合 607 489\n",
      "3000 往往 往往 366 345\n",
      "4000 007 007 255 156\n",
      "5000 處於 處於 192 188\n",
      "6000 一般來說 一般來說 154 150\n",
      "7000 何 何 125 101\n",
      "8000 有鬼 有鬼 105 100\n",
      "9000 備 備 89 89\n",
      "10000 半年前 半年前 78 78\n",
      "11000 掉下來 掉下來 68 66\n",
      "12000 托爾 托爾 61 46\n",
      "13000 傻子 傻子 54 51\n",
      "14000 活得 活得 49 48\n",
      "15000 寫給 寫給 44 41\n",
      "16000 希斯 希斯 41 31\n",
      "17000 毆打 毆打 37 33\n",
      "18000 編制 編制 34 29\n",
      "19000 激怒 激怒 32 30\n",
      "20000 神秘性 神秘性 30 5\n",
      "21000 睡意 睡意 27 27\n",
      "22000 真情 真情 26 24\n",
      "23000 寫些 寫些 24 24\n",
      "24000 野蠻女友 野蠻女友 23 14\n",
      "25000 20th 20th 21 17\n",
      "26000 世家 世家 20 20\n",
      "27000 天降 天降 19 18\n",
      "28000 人卡 人卡 18 13\n",
      "29000 決定權 決定權 17 17\n",
      "30000 始於 始於 16 15\n",
      "31000 初號機 初號機 16 10\n",
      "32000 早睡早起 早睡早起 15 15\n",
      "33000 記號 記號 14 13\n",
      "34000 內力 內力 14 11\n",
      "35000 大蓋 大蓋 13 12\n",
      "36000 冬訓 冬訓 12 12\n",
      "37000 著書 著書 12 12\n",
      "38000 反璞歸真 反璞歸真 11 11\n",
      "39000 皮皮 皮皮 11 8\n",
      "40000 內布拉斯加 內布拉斯加 11 7\n",
      "41000 之翼 之翼 10 10\n",
      "42000 克蘿伊摩蕾茲 克蘿伊摩蕾茲 10 8\n",
      "43000 高崎 高崎 9 5\n",
      "44000 無恥之徒 無恥之徒 9 9\n",
      "45000 散光 散光 9 6\n",
      "46000 XV XV 9 9\n",
      "47000 一王 一王 8 8\n",
      "48000 瑞士刀 瑞士刀 8 4\n",
      "49000 驚字塔 驚字塔 8 8\n",
      "50000 祝您 祝您 8 7\n",
      "51000 先列 先列 7 7\n",
      "52000 先拍 先拍 7 6\n",
      "53000 犯生 犯生 7 7\n",
      "54000 毒發 毒發 7 7\n",
      "55000 水天 水天 6 3\n",
      "56000 隱居 隱居 6 6\n",
      "57000 督個 督個 6 4\n",
      "58000 不太穩 不太穩 6 6\n",
      "59000 錢欣郁 錢欣郁 6 3\n",
      "60000 鑑別 鑑別 6 6\n",
      "61000 歐女 歐女 6 2\n",
      "62000 南丁格爾 南丁格爾 5 5\n",
      "63000 ZARA ZARA 5 5\n",
      "64000 看食 看食 5 5\n",
      "65000 同人小說 同人小說 5 5\n",
      "66000 Hynix Hynix 5 5\n",
      "67000 入浴 入浴 5 4\n",
      "68000 說凜 說凜 5 5\n",
      "69000 還會將 還會將 4 4\n",
      "70000 了然 了然 4 4\n",
      "71000 找錢 找錢 4 4\n",
      "72000 三番 三番 4 4\n",
      "73000 跑者 跑者 4 4\n",
      "74000 Innovation Innovation 4 4\n",
      "75000 組起來 組起來 4 4\n",
      "76000 學不太到 學不太到 4 4\n",
      "77000 鳥氣 鳥氣 4 4\n",
      "78000 武術指導 武術指導 4 4\n",
      "79000 星科晶朋 星科晶朋 4 4\n",
      "80000 dia dia 4 2\n",
      "81000 賴還 賴還 4 4\n",
      "82000 人拿錯 人拿錯 4 4\n",
      "83000 推新 推新 3 3\n",
      "84000 惟此 惟此 3 3\n",
      "85000 蠻近 蠻近 3 3\n",
      "86000 正賽 正賽 3 3\n",
      "87000 唇槍 唇槍 3 3\n",
      "88000 函給 函給 3 3\n",
      "89000 雨女 雨女 3 3\n",
      "90000 狂問 狂問 3 3\n",
      "91000 沒笑點 沒笑點 3 3\n",
      "92000 不掛 不掛 3 3\n",
      "93000 患憂 患憂 3 3\n",
      "94000 Ginoisgood Ginoisgood 3 3\n",
      "95000 thought thought 3 3\n",
      "96000 金陽 金陽 3 3\n",
      "97000 騷靈 騷靈 3 3\n",
      "98000 ATK ATK 3 3\n",
      "99000 realtimenews realtimenews 3 3\n",
      "100000 上展 上展 3 2\n",
      "101000 軟而 軟而 3 3\n",
      "102000 性文多 性文多 3 3\n",
      "103000 狂沙中 狂沙中 2 2\n",
      "104000 友尚 友尚 2 2\n",
      "105000 上序 上序 2 2\n",
      "106000 可增 可增 2 2\n",
      "107000 伊神 伊神 2 2\n",
      "108000 總覽 總覽 2 1\n",
      "109000 大可大辣 大可大辣 2 2\n",
      "110000 Karsa Karsa 2 2\n",
      "111000 老沙 老沙 2 2\n",
      "112000 三連霸 三連霸 2 2\n",
      "113000 人錢 人錢 2 2\n",
      "114000 小縫 小縫 2 2\n",
      "115000 heartdrunken heartdrunken 2 2\n",
      "116000 而妮 而妮 2 2\n",
      "117000 降職 降職 2 1\n",
      "118000 弄手 弄手 2 2\n",
      "119000 跟織 跟織 2 2\n",
      "120000 那聲 那聲 2 2\n",
      "121000 臭氣沖天 臭氣沖天 2 2\n",
      "122000 約聘職 約聘職 2 2\n",
      "123000 亞伯特 亞伯特 2 2\n",
      "124000 母子之情 母子之情 2 2\n",
      "125000 一撥 一撥 2 2\n",
      "126000 foreigner foreigner 2 2\n",
      "127000 沒兵 沒兵 2 2\n",
      "128000 速到 速到 2 2\n",
      "129000 119999999119999991199911999999911991199 119999999119999991199911999999911991199 2 2\n",
      "130000 愛主 愛主 2 2\n",
      "131000 業務經理 業務經理 2 2\n",
      "132000 舒涵 舒涵 2 2\n",
      "133000 浮台 浮台 2 1\n",
      "134000 和吳 和吳 2 2\n",
      "135000 Gatebox Gatebox 2 1\n",
      "136000 whatthefack whatthefack 2 2\n",
      "137000 邊默數 邊默數 2 2\n",
      "138000 李芳 李芳 2 1\n",
      "139000 群后 群后 2 2\n",
      "140000 找外 找外 2 2\n",
      "141000 防空壕 防空壕 2 2\n",
      "142000 道德哲學 道德哲學 1 1\n",
      "143000 Footlight Footlight 1 1\n",
      "144000 Ds Ds 1 1\n",
      "145000 全廢 全廢 1 1\n",
      "146000 喪夫 喪夫 1 1\n",
      "147000 櫻看 櫻看 1 1\n",
      "148000 DOCtRINE DOCtRINE 1 1\n",
      "149000 曼今 曼今 1 1\n",
      "150000 NanoDesu NanoDesu 1 1\n",
      "151000 不佳來 不佳來 1 1\n",
      "152000 重禮 重禮 1 1\n",
      "153000 帶冷 帶冷 1 1\n",
      "154000 繳為 繳為 1 1\n",
      "155000 乃讀 乃讀 1 1\n",
      "156000 附好 附好 1 1\n",
      "157000 無病 無病 1 1\n",
      "158000 1JPmnQcb 1JPmnQcb 1 1\n",
      "159000 他博 他博 1 1\n",
      "160000 歸頭 歸頭 1 1\n",
      "161000 診斷書 診斷書 1 1\n",
      "162000 腳刀 腳刀 1 1\n",
      "163000 比眾 比眾 1 1\n",
      "164000 Awakening Awakening 1 1\n",
      "165000 過卻 過卻 1 1\n",
      "166000 懂講 懂講 1 1\n",
      "167000 Goodwin Goodwin 1 1\n",
      "168000 Therese Therese 1 1\n",
      "169000 歪曲 歪曲 1 1\n",
      "170000 Nawi Nawi 1 1\n",
      "171000 晃棄 晃棄 1 1\n",
      "172000 測試版 測試版 1 1\n",
      "173000 到明 到明 1 1\n",
      "174000 從卡池 從卡池 1 1\n",
      "175000 原美緒 原美緒 1 1\n",
      "176000 戰速 戰速 1 1\n",
      "177000 Ciotta Ciotta 1 1\n",
      "178000 皮有 皮有 1 1\n",
      "179000 頭用 頭用 1 1\n",
      "180000 雙雄片 雙雄片 1 1\n",
      "181000 還辜 還辜 1 1\n",
      "182000 亂當 亂當 1 1\n",
      "183000 給我畫 給我畫 1 1\n",
      "184000 小呆鼠 小呆鼠 1 1\n",
      "185000 買才行 買才行 1 1\n",
      "186000 Trends Trends 1 1\n",
      "187000 立項 立項 1 1\n",
      "188000 岬灣 岬灣 1 1\n",
      "189000 f15 f15 1 1\n",
      "190000 資源配置 資源配置 1 1\n",
      "191000 ahqgogo ahqgogo 1 1\n",
      "192000 Omaya Omaya 1 1\n",
      "193000 人點 人點 1 1\n",
      "194000 不愛買 不愛買 1 1\n",
      "195000 船速 船速 1 1\n",
      "196000 白花油 白花油 1 1\n",
      "197000 人多活 人多活 1 1\n",
      "198000 櫻會 櫻會 1 1\n",
      "199000 偶戲 偶戲 1 1\n"
     ]
    }
   ],
   "source": [
    "# PTT title generator from content base on seq2seq model.\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import types \n",
    "\n",
    "#Initialize for files name and path.\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "totalboard_name = 'TJ-BG-AT-SX-CC-MV'\n",
    "board_name = ['Tech_job','Boy-Girl','AllTogether','sex','C_Chat','movie']\n",
    "board_dict_index = {'Tech_job':0,'Boy-Girl':1,'AllTogether':2,'sex':3,'C_Chat':4,'movie':5}\n",
    "max_word_length = 256\n",
    "min_word_length = 100\n",
    "max_title_length = 20\n",
    "label_newnum = len(board_name)\n",
    "\n",
    "new_content_dir = []\n",
    "totalboard_dir = os.path.join(base_dir, totalboard_name)\n",
    "if not os.path.exists(totalboard_dir):\n",
    "    os.makedirs(totalboard_dir)\n",
    "for name_ind in range(len(board_name)):\n",
    "    # New data path\n",
    "    new_content_dir.append(os.path.join(totalboard_dir, board_name[name_ind]+'_content'))\n",
    "    if not os.path.exists(new_content_dir[name_ind]):\n",
    "        os.makedirs(new_content_dir[name_ind])\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import jieba\n",
    "\n",
    "# Import dictionary fron jieba and PTT.\n",
    "jieba.set_dictionary('C:/Users/User/Anaconda3/Lib/site-packages/jieba/dict.txt.big')\n",
    "jieba.load_userdict('C:/Users/User/Anaconda3/Lib/site-packages/jieba/userdict.txt')\n",
    "\n",
    "text_content = []\n",
    "text_label = []\n",
    "text_title = []\n",
    "\n",
    "GO_word = '<GO>' #解碼器端的句子起始標識符。\n",
    "EOS_word = '<EOS>' #解碼器端的句子結束標識符。\n",
    "PAD_word = '<PAD>' #補全字符。\n",
    "UNK_word = '<UNK>' #低頻詞或者一些未遇到過的詞等。\n",
    "\n",
    "# Load PTT content and information.\n",
    "#dfs_total = pd.read_csv(os.path.join(totalboard_dir, totalboard_name + '-test.csv'))\n",
    "dfs_total = pd.read_csv(os.path.join(totalboard_dir, totalboard_name + '.csv'))\n",
    "len_total = len(dfs_total)\n",
    "\n",
    "for dfs_index in range(len_total):\n",
    "    read_index = int(dfs_total.iloc[dfs_index, 0])\n",
    "    type_name = str(dfs_total.iloc[dfs_index, 1])\n",
    "    word_length = int(dfs_total.iloc[dfs_index, 5])\n",
    "    title_name = str(dfs_total.iloc[dfs_index, 9])\n",
    "    this_board_name = str(dfs_total.iloc[dfs_index, 10])\n",
    "    \n",
    "    # Add start and end character.\n",
    "    text = GO_word + ' '\n",
    "    with open(os.path.join(new_content_dir[board_dict_index[this_board_name]], str(read_index) + '.csv'),\n",
    "                'r', encoding = 'utf-8-sig') as file:\n",
    "        csvCursor = csv.reader(file)\n",
    "        for rows in csvCursor:\n",
    "            for row in rows:\n",
    "                # Read content and remove empty.\n",
    "                text = text + row\n",
    "        else:\n",
    "            text = text + ' ' + EOS_word\n",
    "    \n",
    "    # Split titles to words by Jieba.\n",
    "    title_name.encode('utf-8-sig')\n",
    "    title_jieba = jieba.cut(title_name, cut_all=False)\n",
    "    title_name = GO_word\n",
    "    word_length = 0\n",
    "    for word in title_jieba:\n",
    "        word_length += 1\n",
    "        title_name = title_name + ' ' + word\n",
    "    else:\n",
    "        title_name = title_name + ' ' + EOS_word\n",
    "\n",
    "    text_content.append(text)\n",
    "    text_label.append(board_dict_index[this_board_name])\n",
    "    text_title.append(title_name)\n",
    "    file.close()\n",
    "\n",
    "# Build vocabulary and convert content to sequence by Keras tool.\n",
    "words_limit = 60000\n",
    "tokenizer = Tokenizer(num_words=words_limit, \n",
    "                      filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n　，。！：；、？﹝﹞「」『』（）｛｝［］【】《》“”‘’＼｜〝〞‵′＋－＊／＝≦≧＿＠＃＄％︿＆～§◎．※ㄧ↔│○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓╱╲╳▁▂▄▅▆▇█▏▎▍▌▋▊▉▁▔', \n",
    "                      lower=False,\n",
    "                      split=\" \")\n",
    "                      #oov_token=UNK_word)\n",
    "tokenizer.fit_on_texts(text_content+text_title)\n",
    "vocab = tokenizer.word_index\n",
    "vocab_counts = tokenizer.word_counts\n",
    "vocab_docs = tokenizer.word_docs\n",
    "\n",
    "print(len(vocab))\n",
    "anti_vocab = {}\n",
    "vocab_keys = list(vocab.keys())\n",
    "for vocab_word in vocab:\n",
    "    vocab_index = vocab[vocab_word]\n",
    "    anti_vocab[vocab_index] = vocab_keys[vocab_index-1]\n",
    "    if vocab_index % 1000 == 0 or vocab_index < 20:\n",
    "        print(vocab[vocab_word], vocab_word, anti_vocab[vocab_index], vocab_counts[vocab_word], vocab_docs[vocab_word])\n",
    "\n",
    "    \n",
    "from keras.utils import np_utils\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_content, text_title, test_size=0.2, random_state=9487)\n",
    "\n",
    "# 将每个词用词典中的数值代替\n",
    "x_train_word_ids = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_word_ids = tokenizer.texts_to_sequences(x_test)\n",
    "y_train_word_ids = tokenizer.texts_to_sequences(y_train)\n",
    "y_test_word_ids = tokenizer.texts_to_sequences(y_test)\n",
    "# 序列模式\n",
    "x_train = pad_sequences(x_train_word_ids, maxlen=max_word_length+2, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test_word_ids, maxlen=max_word_length+2, padding='post', truncating='post')\n",
    "y_train = pad_sequences(y_train_word_ids, maxlen=max_title_length+2, padding='post', truncating='post')\n",
    "y_test = pad_sequences(y_test_word_ids, maxlen=max_title_length+2, padding='post', truncating='post')\n",
    "# 序列長度\n",
    "x_train_size = [len(word_ids) for word_ids in x_train_word_ids]\n",
    "x_test_size = [len(word_ids) for word_ids in x_test_word_ids]\n",
    "y_train_size = [len(word_ids) for word_ids in y_train_word_ids]\n",
    "y_test_size = [len(word_ids) for word_ids in y_test_word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1343,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9160,
     "status": "ok",
     "timestamp": 1517421499174,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "-iUlULwfCdGC",
    "outputId": "3a3b3192-bbe8-4534-81d5-53f2e05b9566",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      "epoch1\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  6.22122 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  5.16646 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  4.78322 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  4.8321 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  4.50483 \t\t\n",
      "Train Loss:  4.50483\n",
      "Test loss:  4.52549 ; Std:  0.14922\n",
      "Target Words: <GO> 男友 是 念舊 的 人 <EOS>\n",
      "Response Words: <GO> 食戟 高潮 人氣 人氣 人氣 ル ル ル ル ル ル ル ル ル ル ル ル ル ル ル ル\n",
      "Save best score!! 4.52549\n",
      "Elapsed time in epoch 1: 585.4376063346863 [s]\n",
      "\n",
      "========\n",
      "epoch2\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  4.36228 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  4.06004 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  4.13509 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  3.99115 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  3.78285 \t\t\n",
      "Train Loss:  3.78285\n",
      "Test loss:  3.83554 ; Std:  0.166666\n",
      "Target Words: <GO> 中南部 真心 尋找 另 一伴 <EOS>\n",
      "Response Words: <GO> 最想 崛起 崛起 師 師 師 師 師 師 師 師 師 師 師 師 師 中心 中心 中心 中心 中心\n",
      "Save best score!! 3.83554\n",
      "Elapsed time in epoch 2: 583.4924094676971 [s]\n",
      "\n",
      "========\n",
      "epoch3\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  3.23442 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  3.87264 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  3.15359 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  3.19029 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  2.96604 \t\t\n",
      "Train Loss:  2.96604\n",
      "Test loss:  3.2693 ; Std:  0.175309\n",
      "Target Words: <GO> 男友 是 念舊 的 人 <EOS>\n",
      "Response Words: <GO> 一名 崛起 崛起 崛起 ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス\n",
      "Save best score!! 3.2693\n",
      "Elapsed time in epoch 3: 578.979248046875 [s]\n",
      "\n",
      "========\n",
      "epoch4\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  3.04179 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  2.95012 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  2.84368 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  2.9671 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  2.90657 \t\t\n",
      "Train Loss:  2.90657\n",
      "Test loss:  2.91048 ; Std:  0.180742\n",
      "Target Words: <GO> 男友 喪父 情緒 不佳 我能 做些 什麼 <EOS>\n",
      "Response Words: <GO> 大立光 崛起 崛起 崛起 ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス\n",
      "Save best score!! 2.91048\n",
      "Elapsed time in epoch 4: 578.4715440273285 [s]\n",
      "\n",
      "========\n",
      "epoch5\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  2.75205 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  2.50694 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  2.68288 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  2.48207 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  2.58762 \t\t\n",
      "Train Loss:  2.58762\n",
      "Test loss:  2.68456 ; Std:  0.18432\n",
      "Target Words: <GO> 貧乳 vs <EOS>\n",
      "Response Words: <GO> 海洋 崛起 崛起 ジ ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス ス\n",
      "Save best score!! 2.68456\n",
      "Elapsed time in epoch 5: 579.2835986614227 [s]\n",
      "\n",
      "========\n",
      "epoch6\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  2.25678 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  2.17856 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  2.35858 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  2.20958 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  2.38233 \t\t\n",
      "Train Loss:  2.38233\n",
      "Test loss:  2.47779 ; Std:  0.189112\n",
      "Target Words: <GO> Fate S N UBW 16 <EOS>\n",
      "Response Words: <GO> 美商 崛起 崛起 崛起 ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ ジ\n",
      "Save best score!! 2.47779\n",
      "Elapsed time in epoch 6: 578.6395275592804 [s]\n",
      "\n",
      "========\n",
      "epoch7\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  2.03238 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  2.26146 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  2.076 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  2.20319 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  1.97991 \t\t\n",
      "Train Loss:  1.97991\n",
      "Test loss:  2.30784 ; Std:  0.193553\n",
      "Target Words: <GO> 提早 一年 工作 與 學歷 的 選擇 <EOS>\n",
      "Response Words: <GO> 絕地 便當 グ グ グ ン ン ン ン ン ン ン ン ン ン ン ン ン ン ン ン\n",
      "Save best score!! 2.30784\n",
      "Elapsed time in epoch 7: 578.0784306526184 [s]\n",
      "\n",
      "========\n",
      "epoch8\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  1.85909 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  2.11719 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  1.98266 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  1.8409 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  1.81007 \t\t\n",
      "Train Loss:  1.81007\n",
      "Test loss:  2.171 ; Std:  0.197196\n",
      "Target Words: <GO> 元 的 結尾 <EOS>\n",
      "Response Words: <GO> 絕地 區 ら ら ら ら ら ら ら ら ら ら ら ら ら ら ら ら ら ら ら\n",
      "Save best score!! 2.171\n",
      "Elapsed time in epoch 8: 578.0631520748138 [s]\n",
      "\n",
      "========\n",
      "epoch9\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  1.84136 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  1.89852 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  1.82471 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  1.69547 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  1.75992 \t\t\n",
      "Train Loss:  1.75992\n",
      "Test loss:  2.06496 ; Std:  0.200027\n",
      "Target Words: <GO> 我該 怎麼辦 <EOS>\n",
      "Response Words: <GO> 絕地 絕命 グ グ グ ン ら ら ら ら ら ら ら ら ら ら ら ら ら ら ら\n",
      "Save best score!! 2.06496\n",
      "Elapsed time in epoch 9: 577.9690692424774 [s]\n",
      "\n",
      "========\n",
      "epoch10\n",
      "========\n",
      "Iteration  216  ( 19.91 %) Train Loss:  1.88456 \t\t\n",
      "Iteration  432  ( 39.91 %) Train Loss:  1.79325 \t\t\n",
      "Iteration  648  ( 59.91 %) Train Loss:  1.71435 \t\t\n",
      "Iteration  864  ( 79.91 %) Train Loss:  1.61649 \t\t\n",
      "Iteration  1080  ( 99.91 %) Train Loss:  1.44349 \t\t\n",
      "Train Loss:  1.44349\n",
      "Test loss:  1.98223 ; Std:  0.207293\n",
      "Target Words: <GO> 男友 是 念舊 的 人 <EOS>\n",
      "Response Words: <GO> 絕地 區 グ グ グ 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單\n",
      "Save best score!! 1.98223\n",
      "Elapsed time in epoch 10: 578.8928241729736 [s]\n",
      "\n",
      "#######\n",
      "Best model\n",
      "#######\n",
      "Stop by early stopping\n",
      "Best score:  1.98223 Beat model:  9\n",
      "Elapsed time in total: 5834.48268866539\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "patience = 0\n",
    "num_epoch = 10\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "rnn_size = 150\n",
    "num_layers = 4\n",
    "embedding_size = 300\n",
    "vocab_size = words_limit\n",
    "limit_generation_len = max_title_length + 2\n",
    "lr = 1e-3\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    ################\n",
    "    # model input tensor\n",
    "    ################\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    validation_batch_size = tf.shape(targets)[0]\n",
    "    \n",
    "    input_seq_len = tf.placeholder(tf.int32, (None,), name='input_seq_len')\n",
    "    target_seq_len = tf.placeholder(tf.int32, (None,), name='target_seq_len')\n",
    "    max_target_seq_len = tf.reduce_max(target_seq_len, name='max_target_seq_len')\n",
    "    \n",
    "    limit_target_seq_len = tf.placeholder(tf.int32, (), name='limit_target_seq_len')\n",
    "    \n",
    "    ################\n",
    "    # Embedding for encoder and decoder\n",
    "    ################\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size]))\n",
    "    encoder_embed_input = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(embeddings, targets)\n",
    "    \n",
    "    ################\n",
    "    # Encoder by LSTM\n",
    "    ################\n",
    "    # LSTM cell\n",
    "    def get_lstm_cell(rnn_size):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size, \n",
    "                                            initializer=tf.random_uniform_initializer(-0.1, 0.1)) \n",
    "        #lstm_cell.add_variable(regularizer=layers.l2_regularizer(0.1))\n",
    "        return lstm_cell\n",
    "\n",
    "    # Struct encoder by LSTM bi-directional multi-layer\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_embed_input, \n",
    "                                                       sequence_length=input_seq_len, \n",
    "                                                       dtype=tf.float32)\n",
    "    \n",
    "    ################\n",
    "    # Decoder by LSTM\n",
    "    ################\n",
    "    # Struct LSTM cell in Decoder\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    # Output fully-connecting layer\n",
    "    output_layer = Dense(vocab_size, \n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1), \n",
    "                         kernel_regularizer=layers.l2_regularizer(0.1))\n",
    "    \n",
    "    # Struct training decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_embed_input,\n",
    "                                                            target_seq_len)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, training_helper,\n",
    "                                                           encoder_state, output_layer) \n",
    "        training_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_target_seq_len)\n",
    "    # Struct predicting decoder (Share variable with training)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([vocab[GO_word]], dtype=tf.int32),\n",
    "                               [validation_batch_size], name='start_tokens')\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                     start_tokens, \n",
    "                                                                     vocab[EOS_word])\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, predicting_helper,\n",
    "                                                             encoder_state, output_layer)\n",
    "        predicting_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,\n",
    "                                                                        impute_finished=True,\n",
    "                                                                        maximum_iterations=limit_target_seq_len)\n",
    "    \n",
    "    ################\n",
    "    # Model output tensor\n",
    "    ################\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, name='trainings')\n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    ################\n",
    "    # Optimization\n",
    "    ################\n",
    "    regularization_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        masks = tf.sequence_mask(target_seq_len, max_target_seq_len, \n",
    "                                 dtype=tf.float32, name='masks')\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        train_step = optimizer.apply_gradients(gradients)\n",
    "        #gradients, vriables = zip(*optimizer.compute_gradients(cost))\n",
    "        #gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        #train_step = optimizer.apply_gradients(zip(gradients, vriables))\n",
    "\n",
    "model_name = 'PTT_Encoder-Decoder_model_fixed_' + str(time.time())\n",
    "mdl_dir = os.path.join(base_dir, 'model')\n",
    "if not os.path.exists(mdl_dir):\n",
    "    os.makedirs(mdl_dir)\n",
    "model_dir = os.path.join(mdl_dir, model_name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "num_instances = len(y_train)\n",
    "iter_per_epoch = int(num_instances / batch_size)\n",
    "iter_pct10 = int(iter_per_epoch / 5)\n",
    "if (num_instances % batch_size) > 0:\n",
    "    iter_per_epoch += 1\n",
    "batch_cutoff = [0]\n",
    "for i in range(iter_per_epoch - 1):\n",
    "    batch_cutoff.append(batch_size * (i+1))\n",
    "batch_cutoff.append(num_instances)\n",
    "\n",
    "test_num_instances = len(y_test)\n",
    "test_iter_per_epoch = int(test_num_instances / test_batch_size)\n",
    "\n",
    "total_start_t = time.time()\n",
    "best_validation_loss = 0.0\n",
    "best_epoch = 1\n",
    "early_stop_counter = 0\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        rand_idxs = np.random.permutation(num_instances)\n",
    "        print('\\n========')\n",
    "        print('epoch' + str(e+1))\n",
    "        print('========')\n",
    "        start_t = time.time()\n",
    "        \n",
    "        #### Get random input batch and training model. ####\n",
    "        for i in range(iter_per_epoch):\n",
    "            X_batch = []\n",
    "            Y_batch = []\n",
    "            X_size = []\n",
    "            Y_size = []\n",
    "            for n in range(batch_cutoff[i],batch_cutoff[i+1]):\n",
    "                X_batch.append(x_train[rand_idxs[n]])\n",
    "                Y_batch.append(y_train[rand_idxs[n]])\n",
    "                X_size.append(x_train_size[rand_idxs[n]])\n",
    "                Y_size.append(y_train_size[rand_idxs[n]])\n",
    "            max_X_size = max(X_size)\n",
    "            max_Y_size = max(Y_size)\n",
    "            X_batch = np.array(X_batch)\n",
    "            Y_batch = np.array(Y_batch)\n",
    "            X_size = np.array(X_size)\n",
    "            Y_size = np.array(Y_size)\n",
    "            \n",
    "            _, loss = sess.run( [train_step, cost],\n",
    "                                {inputs: X_batch[:,:max_X_size], targets: Y_batch[:,:max_Y_size],\n",
    "                                 input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                                 limit_target_seq_len: limit_generation_len})\n",
    "            \n",
    "            if i % iter_pct10 == iter_pct10 - 1:\n",
    "                print('Iteration ',i+1,' (',round(i*100/iter_per_epoch,2),'%) Train Loss: ',\n",
    "                      loss,'\\t\\t',end='\\n')\n",
    "        \n",
    "        print('Train Loss: ',loss)\n",
    "        \n",
    "        #### Get validation input batch and validating model. ####\n",
    "        test_loss = []\n",
    "        for i in range(test_iter_per_epoch):\n",
    "            i_str = i * test_batch_size\n",
    "            i_end = i_str + test_batch_size\n",
    "            \n",
    "            X_batch = np.array(x_test[i_str:i_end])\n",
    "            Y_batch = np.array(y_test[i_str:i_end])\n",
    "            X_size = np.array(x_test_size[i_str:i_end])\n",
    "            Y_size = np.array(y_test_size[i_str:i_end])\n",
    "            max_X_size = max(X_size)\n",
    "            max_Y_size = max(Y_size)\n",
    "            y_loss = sess.run( cost,\n",
    "                                {inputs: X_batch[:,:max_X_size], targets: Y_batch[:,:max_Y_size],\n",
    "                                 input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                                 limit_target_seq_len: limit_generation_len})\n",
    "            test_loss.append(y_loss)\n",
    "        else:\n",
    "            test_loss = np.asarray(test_loss)\n",
    "            validation_loss = test_loss.mean()\n",
    "            validation_loss_std = test_loss.std()\n",
    "        print('Test loss: ',validation_loss,'; Std: ',validation_loss_std)\n",
    "        \n",
    "        #### Generate title form testing content and validating model. ####\n",
    "        answer_logits = sess.run( predicting_logits,\n",
    "                                {inputs: X_batch[:,:max_X_size], targets: Y_batch[:,:max_Y_size], \n",
    "                                 input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                                 limit_target_seq_len: limit_generation_len})\n",
    "        output_index = random.randint(0,test_batch_size-1)\n",
    "        print('Target Words: {}'.format(\" \".join([anti_vocab[i] for i in Y_batch[output_index] if i != 0])))\n",
    "        print('Response Words: {}'.format(\" \".join([anti_vocab[i] for i in answer_logits[output_index] if i != 0])))\n",
    "        \n",
    "        #### Save model weight parameters and information. ####\n",
    "        if validation_loss < best_validation_loss or e == 0:\n",
    "            best_validation_loss = validation_loss\n",
    "            best_epoch = e\n",
    "            early_stop_counter = 0\n",
    "            print('Save best score!! '+str(best_validation_loss))\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print('Count early stop!! '+str(early_stop_counter))\n",
    "\n",
    "        print('Elapsed time in epoch ' + str(e+1) + ': ' + str(time.time() - start_t) + ' [s]')\n",
    "\n",
    "        model_path = os.path.join(model_dir, 'model-%d.h5' %(e+1))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, model_path)\n",
    "        \n",
    "        if patience != 0 and early_stop_counter >= patience:\n",
    "            break\n",
    "\n",
    "print('\\n#######')\n",
    "print('Best model')\n",
    "print('#######')\n",
    "print('Stop by early stopping')\n",
    "print('Best score: ', best_validation_loss, 'Beat model: ', best_epoch)\n",
    "print('Elapsed time in total: ' + str(time.time() - total_start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/User/Raw data/PTT\\model\\PTT_Encoder-Decoder_model_fixed_1522232514.4444828\\model-10.h5\n",
      "Test index:  603\n",
      "Test input Words: <GO> 晚點 打 最近 要 練習 準備 主要 打 world rule 的 easy 跟 打 rank cheese 隱形 模式 跟 一些 個人 線上 排名 挑戰 easy 就是 達成 特殊 要件 製造 煙火 純粹 破關 3 分內 過完 level 500 為 破關 門檻 打字 TOP 加減 玩 電腦 沒 麥克風 所以 用 通訊 軟體 兩個 帳號 手機 對話 電腦 輸入 聲音 偶爾 才 開 語音 因為 嫌 麻煩 <EOS>\n",
      "Target Words: <GO> <EOS>\n",
      "Response Words: <GO> 絕地 區 グ グ 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "model_name = 'PTT_Encoder-Decoder_model_fixed_1522232514.4444828'\n",
    "epoch = 10\n",
    "model_path = os.path.join(base_dir, 'model', model_name, 'model-{}.h5'.format(epoch))\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # 加载模型\n",
    "    loader = tf.train.import_meta_graph(model_path + '.meta')\n",
    "    loader.restore(sess, model_path)\n",
    "\n",
    "    inputs = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    targets = loaded_graph.get_tensor_by_name('targets:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    input_seq_len = loaded_graph.get_tensor_by_name('input_seq_len:0')\n",
    "    target_seq_len = loaded_graph.get_tensor_by_name('target_seq_len:0')\n",
    "    limit_target_seq_len = loaded_graph.get_tensor_by_name('limit_target_seq_len:0')\n",
    "    \n",
    "    #test_num_instances = len(y_test)\n",
    "    #test_iter_per_epoch = int(test_num_instances / test_batch_size)\n",
    "    #val_logits = []\n",
    "    #for i in range(1):\n",
    "    #i_str = i * test_batch_size\n",
    "    i_str = random.randint(0,test_num_instances-test_batch_size-1)\n",
    "    i_end = i_str + test_batch_size\n",
    "\n",
    "    X_batch = np.array(x_test[i_str:i_end])\n",
    "    Y_batch = np.array(y_test[i_str:i_end])\n",
    "    X_size = np.array(x_test_size[i_str:i_end])\n",
    "    Y_size = np.array(y_test_size[i_str:i_end])\n",
    "    max_X_size = max(X_size)\n",
    "    max_Y_size = max(Y_size)\n",
    "    answer_logits = sess.run( logits,\n",
    "                            {inputs: X_batch[:,:max_X_size], targets: Y_batch[:,:max_Y_size], \n",
    "                             input_seq_len: X_size, target_seq_len: Y_size,\n",
    "                             limit_target_seq_len: limit_generation_len})\n",
    "    #val_logits.append(answer_logits)\n",
    "    \n",
    "    output_index = random.randint(0,test_batch_size-1)\n",
    "    print('Test index: ', i_str + output_index)\n",
    "    print('Test input Words: {}'.format(\" \".join([anti_vocab[i] for i in X_batch[output_index] if i != 0])))\n",
    "    print('Target Words: {}'.format(\" \".join([anti_vocab[i] for i in Y_batch[output_index] if i != 0])))\n",
    "    print('Response Words: {}'.format(\" \".join([anti_vocab[i] for i in answer_logits[output_index] if i != 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Test input Words: <GO> 一個 維修 更新 兩次 1 4 > 1 5 > 1 6 官方 說 更新 完在 等 審核 之後 去 更新 跳到 1 5 結果 還是 不能 登入 一直 覺得 納悶 戰鬥 介面 做 了 小 修正 多 了 一個 連擊 傷害 總和 左 下 人物 經驗 條 變成 曲線 型 O o 右邊 人物 頭像 縮小 而且 可以 了 再也 不怕 塞 不下 六個 人 推 其實 我 覺得 在 野外 帶著 pvp 天賦 比 裝備 重要 05 23 15 17 推 1 PVP 天賦 重要 很多 05 23 15 23 greydust 我 覺得 徽章 最 重要 可以 徽章 匕首 來個 一套 05 23 15 23 推 打不贏 就 自殺 是 吧 樓上 05 23 15 40 greydust 打不贏 就 自殺 不夠 專業 一 被 偷襲 就 徽章 解掉 接 匕首 才 專業 05 23 15 41 <EOS>\n",
      "Test output Words: <GO> 境界 之詩 王女 開放 技能 關 <EOS>\n",
      "Response Words: <GO> 絕地 區 グ グ グ 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單 名單\n"
     ]
    }
   ],
   "source": [
    "output_index = random.randint(0,test_batch_size-1)\n",
    "print(output_index)\n",
    "print('Test input Words: {}'.format(\" \".join([anti_vocab[i] for i in X_batch[output_index] if i != 0])))\n",
    "print('Test output Words: {}'.format(\" \".join([anti_vocab[i] for i in Y_batch[output_index] if i != 0])))\n",
    "print('Response Words: {}'.format(\" \".join([anti_vocab[i] for i in answer_logits[output_index] if i != 0])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "multi-PTT text classification model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
